{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Here we will set up all the steps for the AKS cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from environs import Env\n",
    "\n",
    "ENV = Env()\n",
    "ENV.read_env()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = ENV(\"AZURE_SUBSCRIPTION_ID\")\n",
    "resource_group = ENV(\"AZURE_RESOURCE_GROUP\") #  i.e.'kuberg'\n",
    "cluster_name = \"kubeflow-labs-test\"\n",
    "location = 'eastus'\n",
    "agent_size = \"Standard_D1_v2\" # i.e. 'Standard_NC6', Standard_D1_v2\n",
    "aks_name = ENV(\"AKS_NAME\") # i.e. 'kubeaks'\n",
    "agent_count = 2 # agent count is the number of VMs that will be provisioned in the cluster, you can pick any number.\n",
    "storage_account = \"fenocerakubestorage\"#ENV(\"STORAGE_ACCOUNT_NAME\") # i.e. 'kubest'\n",
    "storage_container = ENV(\"AKS_CONTAINER\") # i.e. 'blobfuse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\r\n",
      "  \"id\": \"/subscriptions/fb45cb39-23ee-447d-a047-4c8ba0a5d527/resourceGroups/fenocera_rg_2\",\r\n",
      "  \"location\": \"eastus\",\r\n",
      "  \"managedBy\": null,\r\n",
      "  \"name\": \"fenocera_rg_2\",\r\n",
      "  \"properties\": {\r\n",
      "    \"provisioningState\": \"Succeeded\"\r\n",
      "  },\r\n",
      "  \"tags\": null,\r\n",
      "  \"type\": null\r\n",
      "}\r\n"
     ]
    }
   ],
   "source": [
    "# Create resource group \n",
    "\n",
    "!az group create --name {resource_group} --location {location}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KubernetesVersion    Upgrades\r\n",
      "-------------------  ------------------------\r\n",
      "1.13.5               None available\r\n",
      "1.12.8               1.13.5\r\n",
      "1.12.7               1.12.8, 1.13.5\r\n",
      "1.11.9               1.12.7, 1.12.8\r\n",
      "1.11.8               1.11.9, 1.12.7, 1.12.8\r\n",
      "1.10.13              1.11.8, 1.11.9\r\n",
      "1.10.12              1.10.13, 1.11.8, 1.11.9\r\n",
      "1.9.11               1.10.12, 1.10.13\r\n",
      "1.9.10               1.9.11, 1.10.12, 1.10.13\r\n"
     ]
    }
   ],
   "source": [
    "# Check kubernetes versions \n",
    "\n",
    "!az aks get-versions --location eastus --output table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K{- Finished ..\n",
      "  \"aadProfile\": null,\n",
      "  \"addonProfiles\": null,\n",
      "  \"agentPoolProfiles\": [\n",
      "    {\n",
      "      \"availabilityZones\": null,\n",
      "      \"count\": 3,\n",
      "      \"enableAutoScaling\": null,\n",
      "      \"maxCount\": null,\n",
      "      \"maxPods\": 110,\n",
      "      \"minCount\": null,\n",
      "      \"name\": \"nodepool1\",\n",
      "      \"orchestratorVersion\": \"1.12.7\",\n",
      "      \"osDiskSizeGb\": 100,\n",
      "      \"osType\": \"Linux\",\n",
      "      \"provisioningState\": \"Succeeded\",\n",
      "      \"type\": \"AvailabilitySet\",\n",
      "      \"vmSize\": \"Standard_D1_v2\",\n",
      "      \"vnetSubnetId\": null\n",
      "    }\n",
      "  ],\n",
      "  \"apiServerAuthorizedIpRanges\": null,\n",
      "  \"dnsPrefix\": \"kubeflow-l-fenocerarg2-fb45cb\",\n",
      "  \"enablePodSecurityPolicy\": null,\n",
      "  \"enableRbac\": true,\n",
      "  \"fqdn\": \"kubeflow-l-fenocerarg2-fb45cb-2e64e56f.hcp.eastus.azmk8s.io\",\n",
      "  \"id\": \"/subscriptions/fb45cb39-23ee-447d-a047-4c8ba0a5d527/resourcegroups/fenocera_rg_2/providers/Microsoft.ContainerService/managedClusters/kubeflow-labs-test\",\n",
      "  \"kubernetesVersion\": \"1.12.7\",\n",
      "  \"linuxProfile\": {\n",
      "    \"adminUsername\": \"azureuser\",\n",
      "    \"ssh\": {\n",
      "      \"publicKeys\": [\n",
      "        {\n",
      "          \"keyData\": \"ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQCk7M98oOwEjH25QHIk7M+GsUZagKua/SkVENsov5iSxBvG4MpYiUEjB+066z/rZ3rF3cyzKps0elIrFQN2JSLHjjVBjPz9VGK5WB2LsaPC9kUFvbcgTIiMqe9P1DYGy+oAMQ/PeuL23NGSVAqHuyOHqQLx81k2C57GsLRiODTLl1julT51yXGT2TOfCta2QNUYuy/lTuBvTpqsuZu6tLFT0F2Y8SOjRJX10LeB0M7Cj+9W2ydUpFkYp6Pt1Q+6CNZlKdE7kdW29bae5VXLWbQ/HhC5cp2WptCzxj60h77AcQoWoJktLNqcCjuYUjwtlopSq+K+k0g6TzurziYWsyrl federica@B88L02-19.redmond.corp.microsoft.com\\n\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  },\n",
      "  \"location\": \"eastus\",\n",
      "  \"name\": \"kubeflow-labs-test\",\n",
      "  \"networkProfile\": {\n",
      "    \"dnsServiceIp\": \"10.0.0.10\",\n",
      "    \"dockerBridgeCidr\": \"172.17.0.1/16\",\n",
      "    \"networkPlugin\": \"kubenet\",\n",
      "    \"networkPolicy\": null,\n",
      "    \"podCidr\": \"10.244.0.0/16\",\n",
      "    \"serviceCidr\": \"10.0.0.0/16\"\n",
      "  },\n",
      "  \"nodeResourceGroup\": \"MC_fenocera_rg_2_kubeflow-labs-test_eastus\",\n",
      "  \"provisioningState\": \"Succeeded\",\n",
      "  \"resourceGroup\": \"fenocera_rg_2\",\n",
      "  \"servicePrincipalProfile\": {\n",
      "    \"clientId\": \"0a9b6ef8-86fd-4a6a-8f83-9380c7356d64\",\n",
      "    \"secret\": null\n",
      "  },\n",
      "  \"tags\": null,\n",
      "  \"type\": \"Microsoft.ContainerService/ManagedClusters\"\n",
      "}\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Create AKS cluster\n",
    "\n",
    "!az aks create --node-vm-size {agent_size} --resource-group {resource_group} --name {cluster_name} --location {location} --kubernetes-version 1.12.7 --node-count {agent_count} --generate-ssh-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A different object named kubeflow-labs-test already exists in your kubeconfig file.\r\n",
      "Overwrite? (y/n): "
     ]
    }
   ],
   "source": [
    "# Get the kubeconfig file \n",
    "\n",
    "!az aks get-credentials --name {cluster_name} --resource-group {resource_group}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "daemonset.extensions \"nvidia-device-plugin-daemonset\" created\r\n"
     ]
    }
   ],
   "source": [
    "# NVIDIA PLUGIN\n",
    "\n",
    "!kubectl apply -f https://raw.githubusercontent.com/nvidia/k8s-device-plugin/v1.11/nvidia-device-plugin.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       STATUS    ROLES     AGE       VERSION\r\n",
      "aks-nodepool1-11457415-0   Ready     agent     21m       v1.12.7\r\n",
      "aks-nodepool1-11457415-1   Ready     agent     21m       v1.12.7\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:               aks-nodepool1-11457415-1\r\n",
      "Roles:              agent\r\n",
      "Labels:             agentpool=nodepool1\r\n",
      "                    beta.kubernetes.io/arch=amd64\r\n",
      "                    beta.kubernetes.io/instance-type=Standard_D1_v2\r\n",
      "                    beta.kubernetes.io/os=linux\r\n",
      "                    failure-domain.beta.kubernetes.io/region=eastus\r\n",
      "                    failure-domain.beta.kubernetes.io/zone=1\r\n",
      "                    kubernetes.azure.com/cluster=MC_fenocera_rg_2_kubeflow-labs-test_eastus\r\n",
      "                    kubernetes.io/hostname=aks-nodepool1-11457415-1\r\n",
      "                    kubernetes.io/role=agent\r\n",
      "                    node-role.kubernetes.io/agent=\r\n",
      "                    storageprofile=managed\r\n",
      "                    storagetier=Standard_LRS\r\n",
      "Annotations:        node.alpha.kubernetes.io/ttl=0\r\n",
      "                    volumes.kubernetes.io/controller-managed-attach-detach=true\r\n",
      "CreationTimestamp:  Wed, 29 May 2019 15:06:34 -0400\r\n",
      "Taints:             <none>\r\n",
      "Unschedulable:      false\r\n",
      "Conditions:\r\n",
      "  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\r\n",
      "  ----                 ------  -----------------                 ------------------                ------                       -------\r\n",
      "  NetworkUnavailable   False   Wed, 29 May 2019 15:08:24 -0400   Wed, 29 May 2019 15:08:24 -0400   RouteCreated                 RouteController created a route\r\n",
      "  OutOfDisk            False   Wed, 29 May 2019 15:28:41 -0400   Wed, 29 May 2019 15:06:34 -0400   KubeletHasSufficientDisk     kubelet has sufficient disk space available\r\n",
      "  MemoryPressure       False   Wed, 29 May 2019 15:28:41 -0400   Wed, 29 May 2019 15:06:34 -0400   KubeletHasSufficientMemory   kubelet has sufficient memory available\r\n",
      "  DiskPressure         False   Wed, 29 May 2019 15:28:41 -0400   Wed, 29 May 2019 15:06:34 -0400   KubeletHasNoDiskPressure     kubelet has no disk pressure\r\n",
      "  PIDPressure          False   Wed, 29 May 2019 15:28:41 -0400   Wed, 29 May 2019 15:06:34 -0400   KubeletHasSufficientPID      kubelet has sufficient PID available\r\n",
      "  Ready                True    Wed, 29 May 2019 15:28:41 -0400   Wed, 29 May 2019 15:06:34 -0400   KubeletReady                 kubelet is posting ready status. AppArmor enabled\r\n",
      "Addresses:\r\n",
      "  Hostname:    aks-nodepool1-11457415-1\r\n",
      "  InternalIP:  10.240.0.4\r\n",
      "Capacity:\r\n",
      " attachable-volumes-azure-disk:  4\r\n",
      " cpu:                            1\r\n",
      " ephemeral-storage:              101584140Ki\r\n",
      " hugepages-1Gi:                  0\r\n",
      " hugepages-2Mi:                  0\r\n",
      " memory:                         3500456Ki\r\n",
      " pods:                           110\r\n",
      "Allocatable:\r\n",
      " attachable-volumes-azure-disk:  4\r\n",
      " cpu:                            940m\r\n",
      " ephemeral-storage:              93619943269\r\n",
      " hugepages-1Gi:                  0\r\n",
      " hugepages-2Mi:                  0\r\n",
      " memory:                         1814952Ki\r\n",
      " pods:                           110\r\n",
      "System Info:\r\n",
      " Machine ID:                 8b5c36ebb1de4c19a714b868c35df61b\r\n",
      " System UUID:                49FA0F9D-6244-1C4A-A8B7-8FE88E0A0DE2\r\n",
      " Boot ID:                    26301674-421e-4d67-83dc-a0c65a19ba19\r\n",
      " Kernel Version:             4.15.0-1042-azure\r\n",
      " OS Image:                   Ubuntu 16.04.6 LTS\r\n",
      " Operating System:           linux\r\n",
      " Architecture:               amd64\r\n",
      " Container Runtime Version:  docker://3.0.4\r\n",
      " Kubelet Version:            v1.12.7\r\n",
      " Kube-Proxy Version:         v1.12.7\r\n",
      "PodCIDR:                     10.244.0.0/24\r\n",
      "ExternalID:                  aks-nodepool1-11457415-1\r\n",
      "ProviderID:                  azure:///subscriptions/fb45cb39-23ee-447d-a047-4c8ba0a5d527/resourceGroups/MC_fenocera_rg_2_kubeflow-labs-test_eastus/providers/Microsoft.Compute/virtualMachines/aks-nodepool1-11457415-1\r\n",
      "Non-terminated Pods:         (10 in total)\r\n",
      "  Namespace                  Name                                     CPU Requests  CPU Limits  Memory Requests  Memory Limits\r\n",
      "  ---------                  ----                                     ------------  ----------  ---------------  -------------\r\n",
      "  kube-system                coredns-66cb57b9db-9zhp6                 100m (10%)    0 (0%)      70Mi (3%)        170Mi (9%)\r\n",
      "  kube-system                coredns-66cb57b9db-zbwb6                 100m (10%)    0 (0%)      70Mi (3%)        170Mi (9%)\r\n",
      "  kube-system                coredns-autoscaler-7fd449d848-jzqtp      20m (2%)      0 (0%)      10Mi (0%)        0 (0%)\r\n",
      "  kube-system                heapster-7677c744b8-tbqgn                130m (13%)    130m (13%)  230Mi (12%)      230Mi (12%)\r\n",
      "  kube-system                kube-proxy-mzpvp                         100m (10%)    0 (0%)      0 (0%)           0 (0%)\r\n",
      "  kube-system                kube-svc-redirect-lqdjg                  10m (1%)      0 (0%)      34Mi (1%)        0 (0%)\r\n",
      "  kube-system                kubernetes-dashboard-7b55c6f7b9-5wf59    100m (10%)    100m (10%)  50Mi (2%)        500Mi (28%)\r\n",
      "  kube-system                metrics-server-67c75dbf7-kf2hn           0 (0%)        0 (0%)      0 (0%)           0 (0%)\r\n",
      "  kube-system                nvidia-device-plugin-daemonset-kt7q4     0 (0%)        0 (0%)      0 (0%)           0 (0%)\r\n",
      "  kube-system                tunnelfront-9d4f995d8-qmwth              10m (1%)      0 (0%)      64Mi (3%)        0 (0%)\r\n",
      "Allocated resources:\r\n",
      "  (Total limits may be over 100 percent, i.e., overcommitted.)\r\n",
      "  CPU Requests  CPU Limits  Memory Requests  Memory Limits\r\n",
      "  ------------  ----------  ---------------  -------------\r\n",
      "  570m (60%)    230m (24%)  528Mi (29%)      1070Mi (60%)\r\n",
      "Events:\r\n",
      "  Type    Reason                   Age                From                                  Message\r\n",
      "  ----    ------                   ----               ----                                  -------\r\n",
      "  Normal  Starting                 22m                kubelet, aks-nodepool1-11457415-1     Starting kubelet.\r\n",
      "  Normal  NodeHasSufficientDisk    22m (x2 over 22m)  kubelet, aks-nodepool1-11457415-1     Node aks-nodepool1-11457415-1 status is now: NodeHasSufficientDisk\r\n",
      "  Normal  NodeHasSufficientMemory  22m (x2 over 22m)  kubelet, aks-nodepool1-11457415-1     Node aks-nodepool1-11457415-1 status is now: NodeHasSufficientMemory\r\n",
      "  Normal  NodeHasNoDiskPressure    22m (x2 over 22m)  kubelet, aks-nodepool1-11457415-1     Node aks-nodepool1-11457415-1 status is now: NodeHasNoDiskPressure\r\n",
      "  Normal  NodeHasSufficientPID     22m (x2 over 22m)  kubelet, aks-nodepool1-11457415-1     Node aks-nodepool1-11457415-1 status is now: NodeHasSufficientPID\r\n",
      "  Normal  NodeAllocatableEnforced  22m                kubelet, aks-nodepool1-11457415-1     Updated Node Allocatable limit across pods\r\n",
      "  Normal  NodeReady                22m                kubelet, aks-nodepool1-11457415-1     Node aks-nodepool1-11457415-1 status is now: NodeReady\r\n",
      "  Normal  Starting                 21m                kube-proxy, aks-nodepool1-11457415-1  Starting kube-proxy.\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe node aks-nodepool1-11457415-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "job.batch \"2-mnist-training\" created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl create -f 2-mnist-training.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               DESIRED   SUCCESSFUL   AGE\r\n",
      "2-mnist-training   1         0            1m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                     READY     STATUS              RESTARTS   AGE\r\n",
      "2-mnist-training-dqvrk   0/1       ContainerCreating   0          1m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get po"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-23 17:55:18.035020: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n",
      "2019-05-23 17:55:18.148748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \r\n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\n",
      "pciBusID: 601b:00:00.0\r\n",
      "totalMemory: 11.17GiB freeMemory: 11.11GiB\r\n",
      "2019-05-23 17:55:18.148792: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 601b:00:00.0, compute capability: 3.7)\r\n",
      "2019-05-23 17:55:22.747308: I tensorflow/stream_executor/dso_loader.cc:139] successfully opened CUDA library libcupti.so.8.0 locally\r\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\r\n",
      "Extracting /tmp/tensorflow/input_data/train-images-idx3-ubyte.gz\r\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\r\n",
      "Extracting /tmp/tensorflow/input_data/train-labels-idx1-ubyte.gz\r\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\r\n",
      "Extracting /tmp/tensorflow/input_data/t10k-images-idx3-ubyte.gz\r\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\r\n",
      "Extracting /tmp/tensorflow/input_data/t10k-labels-idx1-ubyte.gz\r\n",
      "Accuracy at step 0: 0.1107\r\n",
      "Accuracy at step 10: 0.733\r\n",
      "Accuracy at step 20: 0.8266\r\n",
      "Accuracy at step 30: 0.8568\r\n",
      "Accuracy at step 40: 0.8653\r\n",
      "Accuracy at step 50: 0.8912\r\n",
      "Accuracy at step 60: 0.8945\r\n",
      "Accuracy at step 70: 0.9019\r\n",
      "Accuracy at step 80: 0.9125\r\n",
      "Accuracy at step 90: 0.9156\r\n",
      "Adding run metadata for 99\r\n",
      "Accuracy at step 100: 0.917\r\n",
      "Accuracy at step 110: 0.9191\r\n",
      "Accuracy at step 120: 0.9187\r\n",
      "Accuracy at step 130: 0.9215\r\n",
      "Accuracy at step 140: 0.9236\r\n",
      "Accuracy at step 150: 0.9293\r\n",
      "Accuracy at step 160: 0.9299\r\n",
      "Accuracy at step 170: 0.9328\r\n",
      "Accuracy at step 180: 0.9293\r\n",
      "Accuracy at step 190: 0.9347\r\n",
      "Adding run metadata for 199\r\n",
      "Accuracy at step 200: 0.9358\r\n",
      "Accuracy at step 210: 0.9356\r\n",
      "Accuracy at step 220: 0.9316\r\n",
      "Accuracy at step 230: 0.9362\r\n",
      "Accuracy at step 240: 0.9382\r\n",
      "Accuracy at step 250: 0.9347\r\n",
      "Accuracy at step 260: 0.943\r\n",
      "Accuracy at step 270: 0.9434\r\n",
      "Accuracy at step 280: 0.9462\r\n",
      "Accuracy at step 290: 0.9451\r\n",
      "Adding run metadata for 299\r\n",
      "Accuracy at step 300: 0.9447\r\n",
      "Accuracy at step 310: 0.9489\r\n",
      "Accuracy at step 320: 0.9455\r\n",
      "Accuracy at step 330: 0.9491\r\n",
      "Accuracy at step 340: 0.9517\r\n",
      "Accuracy at step 350: 0.9483\r\n",
      "Accuracy at step 360: 0.9498\r\n",
      "Accuracy at step 370: 0.9513\r\n",
      "Accuracy at step 380: 0.9492\r\n",
      "Accuracy at step 390: 0.9512\r\n",
      "Adding run metadata for 399\r\n",
      "Accuracy at step 400: 0.9512\r\n",
      "Accuracy at step 410: 0.9532\r\n",
      "Accuracy at step 420: 0.9522\r\n",
      "Accuracy at step 430: 0.9549\r\n",
      "Accuracy at step 440: 0.9546\r\n",
      "Accuracy at step 450: 0.9563\r\n",
      "Accuracy at step 460: 0.9553\r\n",
      "Accuracy at step 470: 0.9565\r\n",
      "Accuracy at step 480: 0.9552\r\n",
      "Accuracy at step 490: 0.9522\r\n",
      "Adding run metadata for 499\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs 2-mnist-training-dqvrk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME               DESIRED   SUCCESSFUL   AGE\r\n",
      "2-mnist-training   1         1            16m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl get pod --all-namespaces | grep tiller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount \"tiller\" unchanged\n",
      "clusterrolebinding.rbac.authorization.k8s.io \"tiller\" configured\n"
     ]
    }
   ],
   "source": [
    "# If there is no tiller running have to install/set up help and tiller in the aks cluster \n",
    "\n",
    "!kubectl apply -f helm-rbac.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$HELM_HOME has been configured at /Users/federica/.helm.\n",
      "Warning: Tiller is already installed in the cluster.\n",
      "(Use --client-only to suppress this message, or --upgrade to upgrade Tiller to the current version.)\n",
      "Happy Helming!\n"
     ]
    }
   ],
   "source": [
    "!helm init --service-account tiller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kube-system   tiller-deploy-7b65c7bff9-fndck              1/1       Running   0          18h\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pod --all-namespaces | grep tiller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:   named-cardinal\n",
      "LAST DEPLOYED: Thu May 23 14:32:53 2019\n",
      "NAMESPACE: default\n",
      "STATUS: DEPLOYED\n",
      "\n",
      "RESOURCES:\n",
      "==> v1/ConfigMap\n",
      "NAME                          DATA  AGE\n",
      "named-cardinal-mariadb        1     0s\n",
      "named-cardinal-mariadb-tests  1     0s\n",
      "\n",
      "==> v1/Deployment\n",
      "NAME                      READY  UP-TO-DATE  AVAILABLE  AGE\n",
      "named-cardinal-wordpress  0/1    1           0          0s\n",
      "\n",
      "==> v1/PersistentVolumeClaim\n",
      "NAME                      STATUS   VOLUME   CAPACITY  ACCESS MODES  STORAGECLASS  AGE\n",
      "named-cardinal-wordpress  Pending  default  0s\n",
      "\n",
      "==> v1/Pod(related)\n",
      "NAME                                       READY  STATUS   RESTARTS  AGE\n",
      "named-cardinal-mariadb-0                   0/1    Pending  0         0s\n",
      "named-cardinal-wordpress-5456db8f66-8nnsh  0/1    Pending  0         0s\n",
      "\n",
      "==> v1/Secret\n",
      "NAME                      TYPE    DATA  AGE\n",
      "named-cardinal-mariadb    Opaque  2     0s\n",
      "named-cardinal-wordpress  Opaque  1     0s\n",
      "\n",
      "==> v1/Service\n",
      "NAME                      TYPE          CLUSTER-IP   EXTERNAL-IP  PORT(S)                     AGE\n",
      "named-cardinal-mariadb    ClusterIP     10.0.61.195  <none>       3306/TCP                    0s\n",
      "named-cardinal-wordpress  LoadBalancer  10.0.213.40  <pending>    80:30691/TCP,443:31722/TCP  0s\n",
      "\n",
      "==> v1beta1/StatefulSet\n",
      "NAME                    READY  AGE\n",
      "named-cardinal-mariadb  0/1    0s\n",
      "\n",
      "\n",
      "NOTES:\n",
      "1. Get the WordPress URL:\n",
      "\n",
      "  NOTE: It may take a few minutes for the LoadBalancer IP to be available.\n",
      "        Watch the status with: 'kubectl get svc --namespace default -w named-cardinal-wordpress'\n",
      "  export SERVICE_IP=$(kubectl get svc --namespace default named-cardinal-wordpress --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\")\n",
      "  echo \"WordPress URL: http://$SERVICE_IP/\"\n",
      "  echo \"WordPress Admin URL: http://$SERVICE_IP/admin\"\n",
      "\n",
      "2. Login with the following credentials to see your blog\n",
      "\n",
      "  echo Username: user\n",
      "  echo Password: $(kubectl get secret --namespace default named-cardinal-wordpress -o jsonpath=\"{.data.wordpress-password}\" | base64 --decode)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!helm install stable/wordpress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating my_helm_chart\r\n"
     ]
    }
   ],
   "source": [
    "# To create your own helm chart\n",
    "\n",
    "!helm create my_helm_chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To deploy your helm chart to the cluster\n",
    "\n",
    "!helm install . --name my-custom-chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:   flippant-rat\n",
      "LAST DEPLOYED: Thu May 23 14:59:05 2019\n",
      "NAMESPACE: default\n",
      "STATUS: DEPLOYED\n",
      "\n",
      "RESOURCES:\n",
      "==> v1/PersistentVolumeClaim\n",
      "NAME                            STATUS   VOLUME   CAPACITY  ACCESS MODES  STORAGECLASS  AGE\n",
      "flippant-rat-dokuwiki-apache    Pending  default  1s\n",
      "flippant-rat-dokuwiki-dokuwiki  Pending  default  1s\n",
      "\n",
      "==> v1/Pod(related)\n",
      "NAME                                   READY  STATUS   RESTARTS  AGE\n",
      "flippant-rat-dokuwiki-8fbfddd6f-bdmxw  0/1    Pending  0         1s\n",
      "\n",
      "==> v1/Secret\n",
      "NAME                   TYPE    DATA  AGE\n",
      "flippant-rat-dokuwiki  Opaque  1     1s\n",
      "\n",
      "==> v1/Service\n",
      "NAME                   TYPE          CLUSTER-IP  EXTERNAL-IP  PORT(S)                     AGE\n",
      "flippant-rat-dokuwiki  LoadBalancer  10.0.7.53   <pending>    80:31415/TCP,443:31901/TCP  1s\n",
      "\n",
      "==> v1beta1/Deployment\n",
      "NAME                   READY  UP-TO-DATE  AVAILABLE  AGE\n",
      "flippant-rat-dokuwiki  0/1    1           0          1s\n",
      "\n",
      "\n",
      "NOTES:\n",
      "\n",
      "** Please be patient while the chart is being deployed **\n",
      "\n",
      "1. Get the DokuWiki URL by running:\n",
      "\n",
      "** Please ensure an external IP is associated to the flippant-rat-dokuwiki service before proceeding **\n",
      "** Watch the status using: kubectl get svc --namespace default -w flippant-rat-dokuwiki **\n",
      "\n",
      "  export SERVICE_IP=$(kubectl get svc --namespace default flippant-rat-dokuwiki --template \"{{ range (index .status.loadBalancer.ingress 0) }}{{.}}{{ end }}\")\n",
      "  echo \"URL: http://$SERVICE_IP/\"\n",
      "\n",
      "2. Login with the following credentials\n",
      "\n",
      "  echo Username: user\n",
      "  echo Password: $(kubectl get secret --namespace default flippant-rat-dokuwiki -o jsonpath=\"{.data.dokuwiki-password}\" | base64 --decode)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!helm install stable/dokuwiki --set dokuwikiWikiName=\"Hello FED\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                    TYPE           CLUSTER-IP   EXTERNAL-IP    PORT(S)                      AGE\n",
      "flippant-rat-dokuwiki   LoadBalancer   10.0.7.53    13.82.31.171   80:31415/TCP,443:31901/TCP   49m\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!kubectl get svc --namespace default -w flippant-rat-dokuwiki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: /Users/federica/Documents/GitHub/kubeflow: File exists\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100    15  100    15    0     0     97      0 --:--:-- --:--:-- --:--:--    98\n",
      "bash: line 1: 404:: command not found\n"
     ]
    }
   ],
   "source": [
    "KUBEFLOW_SRC = \"Users/federica/Documents/Github/kubeflow-labs/kubeflow\"\n",
    "!mkdir {KUBEFLOW_SRC}\n",
    "!cd {KUBEFLOW_SRC}\n",
    "!export KUBEFLOW_TAG=v0.4.1\n",
    "!curl https://raw.githubusercontent.com/kubeflow/kubeflow/{KUBEFLOW_TAG}/scripts/download.sh | bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: Users/federica/Documents/Github/kubeflow-labs/kubeflow/scripts/kfctl.sh: No such file or directory\n",
      "/bin/sh: Users/federica/Documents/Github/kubeflow-labs/kubeflow/scripts/kfctl.sh: No such file or directory\n",
      "/bin/sh: Users/federica/Documents/Github/kubeflow-labs/kubeflow/scripts/kfctl.sh: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "KFAPP=\"mykubeflowapp\"\n",
    "!{KUBEFLOW_SRC}/scripts/kfctl.sh init {KFAPP} --platform none\n",
    "!cd {KUBEFLOW_SRC}\n",
    "!{KUBEFLOW_SRC}/scripts/kfctl.sh generate k8s\n",
    "!{KUBEFLOW_SRC}/scripts/kfctl.sh apply k8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KFAPP=mykubeflowapp\n",
    "!{KUBEFLOW_SRC}/scripts/kfctl.sh init {KFAPP} --platform none\n",
    "\n",
    "# Generate kubeflow app\n",
    "!cd {KFAPP}\n",
    "!{KUBEFLOW_SRC}/scripts/kfctl.sh generate k8s\n",
    "\n",
    "# Deploy Kubeflow app\n",
    "!{KUBEFLOW_SRC}/scripts/kfctl.sh apply k8s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                        READY     STATUS              RESTARTS   AGE\r\n",
      "ambassador-5cdff47f4d-cdhxv                                 0/1       Running             0          1m\r\n",
      "ambassador-5cdff47f4d-cxxv8                                 1/1       Running             0          1m\r\n",
      "ambassador-5cdff47f4d-lw6wt                                 1/1       Running             0          1m\r\n",
      "argo-ui-789c8577d5-rg4xh                                    1/1       Running             0          1m\r\n",
      "centraldashboard-6f9948dc6d-ll697                           0/1       ContainerCreating   0          1m\r\n",
      "jupyter-0                                                   1/1       Running             0          1m\r\n",
      "jupyter-web-app-5c64c4f4cb-bp72s                            1/1       Running             0          1m\r\n",
      "katib-ui-78f445bf8f-z779c                                   1/1       Running             0          1m\r\n",
      "metacontroller-0                                            1/1       Running             0          1m\r\n",
      "minio-648694bc46-xvnw5                                      0/1       ContainerCreating   0          1m\r\n",
      "ml-pipeline-6c5d5ddbdf-qbjpt                                1/1       Running             0          1m\r\n",
      "ml-pipeline-persistenceagent-5657c6f4bd-6t9s2               1/1       Running             0          1m\r\n",
      "ml-pipeline-scheduledworkflow-6fd8894f86-vq47j              1/1       Running             0          1m\r\n",
      "ml-pipeline-ui-65868fd6cb-hn8nz                             0/1       ContainerCreating   0          1m\r\n",
      "ml-pipeline-viewer-controller-deployment-6c75f9bbfb-cv8xj   1/1       Running             0          1m\r\n",
      "mykubeflowapp-controller-579bd7848f-mzbr7                   1/1       Running             0          1m\r\n",
      "mysql-6f5c96c765-88sdt                                      0/1       ContainerCreating   0          1m\r\n",
      "notebooks-controller-69b6bf4854-5nvwk                       1/1       Running             0          1m\r\n",
      "profiles-77f98bb9cd-qscpz                                   1/1       Running             0          1m\r\n",
      "pytorch-operator-78cc45b958-fg52l                           0/1       ContainerCreating   0          1m\r\n",
      "spartakus-volunteer-7b57fcb8bb-qgtp6                        1/1       Running             0          1m\r\n",
      "studyjob-controller-559f688558-54kpj                        0/1       ContainerCreating   0          1m\r\n",
      "tf-job-dashboard-8fc7bfbc7-92tvz                            0/1       ContainerCreating   0          1m\r\n",
      "tf-job-operator-58fd8db6bd-vppx2                            0/1       ContainerCreating   0          1m\r\n",
      "vizier-core-5dc4594c4c-l2m66                                0/1       ContainerCreating   0          1m\r\n",
      "vizier-core-rest-5fcf9bccf-5nm5z                            0/1       ContainerCreating   0          1m\r\n",
      "vizier-db-cf8cfdc79-px4hf                                   0/1       ContainerCreating   0          1m\r\n",
      "vizier-suggestion-bayesianoptimization-67f69667b9-rhv22     0/1       ContainerCreating   0          1m\r\n",
      "vizier-suggestion-grid-85f9f9d977-5m8vh                     1/1       Running             0          1m\r\n",
      "vizier-suggestion-hyperband-8bcc744b5-fhfjz                 0/1       ContainerCreating   0          1m\r\n",
      "vizier-suggestion-random-6d65dd6645-7mxs7                   0/1       ContainerCreating   0          1m\r\n",
      "workflow-controller-659f5c5dfd-89bvr                        0/1       ContainerCreating   0          1m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods -n kubeflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete deployment \n",
    "\n",
    "!cd {KUBEFLOW_SRC}/{KFAPP}\n",
    "!{KUBEFLOW_SRC}/scripts/kfctl.sh delete k8s # need to be in the app directory (as above suggests)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are now going to run Jupyter on Kubernetes\n",
    "\n",
    "# We run this again to get the deployment up from the app container\n",
    "!../scripts/kfctl.sh apply k8s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                     TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)             AGE\r\n",
      "ambassador                               ClusterIP   10.0.49.249    <none>        80/TCP              1m\r\n",
      "ambassador-admin                         ClusterIP   10.0.132.181   <none>        8877/TCP            1m\r\n",
      "argo-ui                                  NodePort    10.0.25.117    <none>        80:30013/TCP        1m\r\n",
      "centraldashboard                         ClusterIP   10.0.155.216   <none>        80/TCP              1m\r\n",
      "jupyter-0                                ClusterIP   None           <none>        8000/TCP            1m\r\n",
      "jupyter-lb                               ClusterIP   10.0.10.240    <none>        80/TCP              1m\r\n",
      "jupyter-web-app                          ClusterIP   10.0.60.253    <none>        80/TCP              1m\r\n",
      "katib-ui                                 ClusterIP   10.0.228.10    <none>        80/TCP              1m\r\n",
      "minio-service                            ClusterIP   10.0.50.127    <none>        9000/TCP            1m\r\n",
      "ml-pipeline                              ClusterIP   10.0.243.194   <none>        8888/TCP,8887/TCP   1m\r\n",
      "ml-pipeline-tensorboard-ui               ClusterIP   10.0.90.189    <none>        80/TCP              1m\r\n",
      "ml-pipeline-ui                           ClusterIP   10.0.218.245   <none>        80/TCP              1m\r\n",
      "mykubeflowapp-controller                 ClusterIP   10.0.64.96     <none>        80/TCP              1m\r\n",
      "mysql                                    ClusterIP   10.0.238.104   <none>        3306/TCP            1m\r\n",
      "notebooks-controller                     ClusterIP   10.0.48.62     <none>        443/TCP             1m\r\n",
      "profiles                                 ClusterIP   10.0.61.7      <none>        443/TCP             1m\r\n",
      "studyjob-controller                      ClusterIP   10.0.237.182   <none>        443/TCP             1m\r\n",
      "tf-job-dashboard                         ClusterIP   10.0.251.73    <none>        80/TCP              1m\r\n",
      "vizier-core                              NodePort    10.0.116.242   <none>        6789:30884/TCP      1m\r\n",
      "vizier-core-rest                         ClusterIP   10.0.4.244     <none>        80/TCP              1m\r\n",
      "vizier-db                                ClusterIP   10.0.138.223   <none>        3306/TCP            1m\r\n",
      "vizier-suggestion-bayesianoptimization   ClusterIP   10.0.195.52    <none>        6789/TCP            1m\r\n",
      "vizier-suggestion-grid                   ClusterIP   10.0.245.174   <none>        6789/TCP            1m\r\n",
      "vizier-suggestion-hyperband              ClusterIP   10.0.47.253    <none>        6789/TCP            1m\r\n",
      "vizier-suggestion-random                 ClusterIP   10.0.61.239    <none>        6789/TCP            1m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get svc -n kubeflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: line 0: cd: ks_app: No such file or directory\n",
      "\u001b[31mERROR\u001b[0m finding app root from starting path: : unable to find ksonnet project \n",
      "/bin/sh: kubeflow/scripts/kfctl.sh: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "# have to go into the ksonnet app part before setting the param\n",
    "\n",
    "!cd ks_app\n",
    "!ks param set jupyter serviceType LoadBalancer\n",
    "!cd ..\n",
    "!kubeflow/scripts/kfctl.sh apply k8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forwarding from 127.0.0.1:8080 -> 80\n",
      "Forwarding from [::1]:8080 -> 80\n",
      "Handling connection for 8080\n",
      "Handling connection for 8080\n",
      "Handling connection for 8080\n",
      "Handling connection for 8080\n",
      "Handling connection for 8080\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Port forward the Kubeflow dashboard so can see whats going on \n",
    "\n",
    "!kubectl port-forward svc/ambassador -n kubeflow 8080:80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       GPU\r\n",
      "aks-nodepool1-11457415-0   <none>\r\n",
      "aks-nodepool1-11457415-1   <none>\r\n",
      "aks-nodepool1-11457415-2   <none>\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get nodes \"-o=custom-columns=NAME:.metadata.name,GPU:.status.allocatable.alpha\\.kubernetes\\.io\\/nvidia-gpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name:               jupyter-0\n",
      "Namespace:          kubeflow\n",
      "Priority:           0\n",
      "PriorityClassName:  <none>\n",
      "Node:               aks-nodepool1-11457415-0/10.240.0.5\n",
      "Start Time:         Fri, 24 May 2019 08:44:57 -0400\n",
      "Labels:             app=jupyter\n",
      "                    app.kubernetes.io/name=mykubeflowapp\n",
      "                    controller-revision-hash=jupyter-54549d585b\n",
      "                    statefulset.kubernetes.io/pod-name=jupyter-0\n",
      "Annotations:        <none>\n",
      "Status:             Running\n",
      "IP:                 10.244.1.51\n",
      "Controlled By:      StatefulSet/jupyter\n",
      "Containers:\n",
      "  jupyter:\n",
      "    Container ID:  docker://78f25ad8923ecc2ed02db7cd3c667a87aa610f8463f5b6c3159abdab0dc79e15\n",
      "    Image:         gcr.io/kubeflow/jupyterhub-k8s:v20180531-3bb991b1\n",
      "    Image ID:      docker-pullable://gcr.io/kubeflow/jupyterhub-k8s@sha256:5e2c71d050bec85c258a31aa4507ca8adb3b2f5158a4dc919a39118b8879a5ce\n",
      "    Ports:         8000/TCP, 8081/TCP\n",
      "    Host Ports:    0/TCP, 0/TCP\n",
      "    Command:\n",
      "      jupyterhub\n",
      "      -f\n",
      "      /etc/config/jupyter_config.py\n",
      "    State:          Running\n",
      "      Started:      Fri, 24 May 2019 08:45:24 -0400\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment:\n",
      "      KF_AUTHENTICATOR:              null\n",
      "      DEFAULT_JUPYTERLAB:            false\n",
      "      STORAGE_CLASS:                 null\n",
      "      ROK_SECRET_NAME:               secret-rok-{username}\n",
      "      KUBERNETES_PORT_443_TCP_ADDR:  kubeflow-l-fenocerarg2-fb45cb-1cf097e8.hcp.eastus.azmk8s.io\n",
      "      KUBERNETES_PORT:               tcp://kubeflow-l-fenocerarg2-fb45cb-1cf097e8.hcp.eastus.azmk8s.io:443\n",
      "      KUBERNETES_PORT_443_TCP:       tcp://kubeflow-l-fenocerarg2-fb45cb-1cf097e8.hcp.eastus.azmk8s.io:443\n",
      "      KUBERNETES_SERVICE_HOST:       kubeflow-l-fenocerarg2-fb45cb-1cf097e8.hcp.eastus.azmk8s.io\n",
      "    Mounts:\n",
      "      /etc/config from config-volume (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from jupyter-token-vl26f (ro)\n",
      "Conditions:\n",
      "  Type              Status\n",
      "  Initialized       True \n",
      "  Ready             True \n",
      "  ContainersReady   True \n",
      "  PodScheduled      True \n",
      "Volumes:\n",
      "  config-volume:\n",
      "    Type:      ConfigMap (a volume populated by a ConfigMap)\n",
      "    Name:      jupyter-config\n",
      "    Optional:  false\n",
      "  jupyter-token-vl26f:\n",
      "    Type:        Secret (a volume populated by a Secret)\n",
      "    SecretName:  jupyter-token-vl26f\n",
      "    Optional:    false\n",
      "QoS Class:       BestEffort\n",
      "Node-Selectors:  <none>\n",
      "Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n",
      "                 node.kubernetes.io/unreachable:NoExecute for 300s\n",
      "Events:\n",
      "  Type    Reason     Age   From                               Message\n",
      "  ----    ------     ----  ----                               -------\n",
      "  Normal  Scheduled  7m    default-scheduler                  Successfully assigned kubeflow/jupyter-0 to aks-nodepool1-11457415-0\n",
      "  Normal  Pulled     7m    kubelet, aks-nodepool1-11457415-0  Container image \"gcr.io/kubeflow/jupyterhub-k8s:v20180531-3bb991b1\" already present on machine\n",
      "  Normal  Created    7m    kubelet, aks-nodepool1-11457415-0  Created container\n",
      "  Normal  Started    7m    kubelet, aks-nodepool1-11457415-0  Started container\n",
      "\n",
      "Name:               jupyter-web-app-5c64c4f4cb-wxg8b\n",
      "Namespace:          kubeflow\n",
      "Priority:           0\n",
      "PriorityClassName:  <none>\n",
      "Node:               aks-nodepool1-11457415-0/10.240.0.5\n",
      "Start Time:         Fri, 24 May 2019 08:44:54 -0400\n",
      "Labels:             app=jupyter-web-app\n",
      "                    app.kubernetes.io/name=mykubeflowapp\n",
      "                    pod-template-hash=5c64c4f4cb\n",
      "Annotations:        <none>\n",
      "Status:             Running\n",
      "IP:                 10.244.1.34\n",
      "Controlled By:      ReplicaSet/jupyter-web-app-5c64c4f4cb\n",
      "Containers:\n",
      "  jupyter-web-app:\n",
      "    Container ID:   docker://9664cfa65ce36303aff5e112b6da5f015ecb5e74be045446bdb6f532e5d1273a\n",
      "    Image:          gcr.io/kubeflow-dev/jupyter-web-app:5e308bf1\n",
      "    Image ID:       docker-pullable://gcr.io/kubeflow-dev/jupyter-web-app@sha256:9a0109eebdb7c5812083f315e0bcfec1bc2c1c652c54348bd1d6d9ed69f3bd58\n",
      "    Port:           5000/TCP\n",
      "    Host Port:      0/TCP\n",
      "    State:          Running\n",
      "      Started:      Fri, 24 May 2019 08:45:23 -0400\n",
      "    Ready:          True\n",
      "    Restart Count:  0\n",
      "    Environment:\n",
      "      ROK_SECRET_NAME:               secret-rok-{username}\n",
      "      UI:                            default\n",
      "      KUBERNETES_PORT_443_TCP_ADDR:  kubeflow-l-fenocerarg2-fb45cb-1cf097e8.hcp.eastus.azmk8s.io\n",
      "      KUBERNETES_PORT:               tcp://kubeflow-l-fenocerarg2-fb45cb-1cf097e8.hcp.eastus.azmk8s.io:443\n",
      "      KUBERNETES_PORT_443_TCP:       tcp://kubeflow-l-fenocerarg2-fb45cb-1cf097e8.hcp.eastus.azmk8s.io:443\n",
      "      KUBERNETES_SERVICE_HOST:       kubeflow-l-fenocerarg2-fb45cb-1cf097e8.hcp.eastus.azmk8s.io\n",
      "    Mounts:\n",
      "      /etc/config from config-volume (rw)\n",
      "      /var/run/secrets/kubernetes.io/serviceaccount from jupyter-web-app-token-t25l8 (ro)\n",
      "Conditions:\n",
      "  Type              Status\n",
      "  Initialized       True \n",
      "  Ready             True \n",
      "  ContainersReady   True \n",
      "  PodScheduled      True \n",
      "Volumes:\n",
      "  config-volume:\n",
      "    Type:      ConfigMap (a volume populated by a ConfigMap)\n",
      "    Name:      jupyter-web-app-config\n",
      "    Optional:  false\n",
      "  jupyter-web-app-token-t25l8:\n",
      "    Type:        Secret (a volume populated by a Secret)\n",
      "    SecretName:  jupyter-web-app-token-t25l8\n",
      "    Optional:    false\n",
      "QoS Class:       BestEffort\n",
      "Node-Selectors:  <none>\n",
      "Tolerations:     node.kubernetes.io/not-ready:NoExecute for 300s\n",
      "                 node.kubernetes.io/unreachable:NoExecute for 300s\n",
      "Events:\n",
      "  Type    Reason     Age   From                               Message\n",
      "  ----    ------     ----  ----                               -------\n",
      "  Normal  Scheduled  7m    default-scheduler                  Successfully assigned kubeflow/jupyter-web-app-5c64c4f4cb-wxg8b to aks-nodepool1-11457415-0\n",
      "  Normal  Pulling    7m    kubelet, aks-nodepool1-11457415-0  pulling image \"gcr.io/kubeflow-dev/jupyter-web-app:5e308bf1\"\n",
      "  Normal  Pulled     7m    kubelet, aks-nodepool1-11457415-0  Successfully pulled image \"gcr.io/kubeflow-dev/jupyter-web-app:5e308bf1\"\n",
      "  Normal  Created    7m    kubelet, aks-nodepool1-11457415-0  Created container\n",
      "  Normal  Started    7m    kubelet, aks-nodepool1-11457415-0  Started container\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kubeflow describe pods jupyter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No resources found.\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get svc -n kubeflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfjob.kubeflow.org \"tfjob\" created\r\n"
     ]
    }
   ],
   "source": [
    "# Making a TFJOB - had to create it with kubeflow.org/v1beta2 rather than v1beta1\n",
    "\n",
    "!kubectl create -f tfjob.yaml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME      AGE\r\n",
      "tfjob     3s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get tfjob #If has a kubernetes namespace then have to use it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       READY     STATUS    RESTARTS   AGE\r\n",
      "named-cardinal-mariadb-0   1/1       Running   0          8m\r\n",
      "tfjob-master-0             1/1       Running   0          5s\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-24 21:57:31.690859: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\r\n",
      "2019-05-24 21:57:31.805821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \r\n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\n",
      "pciBusID: 2bde:00:00.0\r\n",
      "totalMemory: 11.17GiB freeMemory: 11.11GiB\r\n",
      "2019-05-24 21:57:31.805861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 2bde:00:00.0, compute capability: 3.7)\r\n",
      "2019-05-24 21:57:36.557517: I tensorflow/stream_executor/dso_loader.cc:139] successfully opened CUDA library libcupti.so.8.0 locally\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs tfjob-master-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                             STATUS    VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE\r\n",
      "azurefile                        Bound     pvc-0ba98925-7e65-11e9-9597-8ea533332f11   5Gi        RWX            azurefile      43m\r\n",
      "data-named-cardinal-mariadb-0    Bound     pvc-2e75d2ed-7d89-11e9-9597-8ea533332f11   8Gi        RWO            default        1d\r\n",
      "flippant-rat-dokuwiki-apache     Bound     pvc-d78faf92-7d8c-11e9-9597-8ea533332f11   1Gi        RWO            default        1d\r\n",
      "flippant-rat-dokuwiki-dokuwiki   Bound     pvc-d7915238-7d8c-11e9-9597-8ea533332f11   8Gi        RWO            default        1d\r\n",
      "named-cardinal-wordpress         Bound     pvc-2e508c27-7d89-11e9-9597-8ea533332f11   10Gi       RWO            default        1d\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -f "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "storageclass.storage.k8s.io \"azurefile\" configured\n",
      "clusterrole.rbac.authorization.k8s.io \"system:azure-cloud-provider\" configured\n",
      "clusterrolebinding.rbac.authorization.k8s.io \"system:azure-cloud-provider\" configured\n",
      "persistentvolumeclaim \"azurefile\" unchanged\n"
     ]
    }
   ],
   "source": [
    "# Need to create and mount storage\n",
    "\n",
    "!kubectl apply -f azure-file-sc.yaml\n",
    "!kubectl apply -f azure-pvc-roles.yaml\n",
    "!kubectl apply -f azure-file-pvc.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: unable to recognize \"tfjob.yaml\": no matches for kind \"TFJob\" in version \"kubeflow.org/v1beta2\"\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl create -f tfjob.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deleting the deployments we dont want! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       READY     STATUS      RESTARTS   AGE\r\n",
      "named-cardinal-mariadb-0   1/1       Running     1          1h\r\n",
      "tfjob-master-0             0/1       Completed   0          1h\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAMESPACE     NAME                   DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\r\n",
      "kube-system   coredns                2         2         2            2           1d\r\n",
      "kube-system   coredns-autoscaler     1         1         1            1           1d\r\n",
      "kube-system   heapster               1         1         1            1           1d\r\n",
      "kube-system   kubernetes-dashboard   1         1         1            1           1d\r\n",
      "kube-system   metrics-server         1         1         1            1           1d\r\n",
      "kube-system   tiller-deploy          1         1         1            1           1d\r\n",
      "kube-system   tunnelfront            1         1         1            1           1d\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get deployments --all-namespaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"named-cardinal-mariadb-0\" deleted\n",
      "Error from server (NotFound): services \"named-cardinal-mariadb-0\" not found\n",
      "Error from server (NotFound): deployments.extensions \"named-cardinal-mariadb-0\" not found\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete pods,services,deployments named-cardinal-mariadb-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server (NotFound): pods \"named-cardinal-wordpress-5456db8f66-8nnsh\" not found\n",
      "pod \"flippant-rat-dokuwiki-8fbfddd6f-zwtrm\" deleted\n",
      "pod \"named-cardinal-mariadb-0\" deleted\n",
      "pod \"named-cardinal-wordpress-5456db8f66-qdt46\" deleted\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete pod named-cardinal-wordpress-5456db8f66-8nnsh\n",
    "\n",
    "!kubectl delete pods --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: the server doesn't have a resource type \"tfjob-master-0\"\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe tfjob-master-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-24 21:57:31.690859: I tensorflow/core/platform/cpu_feature_guard.cc:137] Your CPU supports instructions that this TensorFlow binary was not compiled to use: SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "2019-05-24 21:57:31.805821: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1030] Found device 0 with properties: \n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
      "pciBusID: 2bde:00:00.0\n",
      "totalMemory: 11.17GiB freeMemory: 11.11GiB\n",
      "2019-05-24 21:57:31.805861: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1120] Creating TensorFlow device (/device:GPU:0) -> (device: 0, name: Tesla K80, pci bus id: 2bde:00:00.0, compute capability: 3.7)\n",
      "2019-05-24 21:57:36.557517: I tensorflow/stream_executor/dso_loader.cc:139] successfully opened CUDA library libcupti.so.8.0 locally\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting /tmp/tensorflow/input_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting /tmp/tensorflow/input_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/tensorflow/input_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/tensorflow/input_data/t10k-labels-idx1-ubyte.gz\n",
      "Accuracy at step 0: 0.0945\n",
      "Accuracy at step 10: 0.6712\n",
      "Accuracy at step 20: 0.8249\n",
      "Accuracy at step 30: 0.8637\n",
      "Accuracy at step 40: 0.8821\n",
      "Accuracy at step 50: 0.8871\n",
      "Accuracy at step 60: 0.8985\n",
      "Accuracy at step 70: 0.9057\n",
      "Accuracy at step 80: 0.9136\n",
      "Accuracy at step 90: 0.9102\n",
      "Adding run metadata for 99\n",
      "Accuracy at step 100: 0.9108\n",
      "Accuracy at step 110: 0.9212\n",
      "Accuracy at step 120: 0.9173\n",
      "Accuracy at step 130: 0.9246\n",
      "Accuracy at step 140: 0.928\n",
      "Accuracy at step 150: 0.9257\n",
      "Accuracy at step 160: 0.9272\n",
      "Accuracy at step 170: 0.9255\n",
      "Accuracy at step 180: 0.9303\n",
      "Accuracy at step 190: 0.9333\n",
      "Adding run metadata for 199\n",
      "Accuracy at step 200: 0.9255\n",
      "Accuracy at step 210: 0.9305\n",
      "Accuracy at step 220: 0.9356\n",
      "Accuracy at step 230: 0.9377\n",
      "Accuracy at step 240: 0.9377\n",
      "Accuracy at step 250: 0.938\n",
      "Accuracy at step 260: 0.9407\n",
      "Accuracy at step 270: 0.9412\n",
      "Accuracy at step 280: 0.9416\n",
      "Accuracy at step 290: 0.9426\n",
      "Adding run metadata for 299\n",
      "Accuracy at step 300: 0.9459\n",
      "Accuracy at step 310: 0.9445\n",
      "Accuracy at step 320: 0.9445\n",
      "Accuracy at step 330: 0.9439\n",
      "Accuracy at step 340: 0.9493\n",
      "Accuracy at step 350: 0.9484\n",
      "Accuracy at step 360: 0.9453\n",
      "Accuracy at step 370: 0.9478\n",
      "Accuracy at step 380: 0.9493\n",
      "Accuracy at step 390: 0.9459\n",
      "Adding run metadata for 399\n",
      "Accuracy at step 400: 0.9482\n",
      "Accuracy at step 410: 0.9538\n",
      "Accuracy at step 420: 0.9547\n",
      "Accuracy at step 430: 0.9514\n",
      "Accuracy at step 440: 0.9542\n",
      "Accuracy at step 450: 0.9521\n",
      "Accuracy at step 460: 0.9542\n",
      "Accuracy at step 470: 0.9505\n",
      "Accuracy at step 480: 0.9543\n",
      "Accuracy at step 490: 0.9572\n",
      "Adding run metadata for 499\n",
      "Accuracy at step 500: 0.9571\n",
      "Accuracy at step 510: 0.9542\n",
      "Accuracy at step 520: 0.9574\n",
      "Accuracy at step 530: 0.958\n",
      "Accuracy at step 540: 0.9573\n",
      "Accuracy at step 550: 0.9579\n",
      "Accuracy at step 560: 0.96\n",
      "Accuracy at step 570: 0.9566\n",
      "Accuracy at step 580: 0.9589\n",
      "Accuracy at step 590: 0.9586\n",
      "Adding run metadata for 599\n",
      "Accuracy at step 600: 0.9615\n",
      "Accuracy at step 610: 0.9599\n",
      "Accuracy at step 620: 0.9626\n",
      "Accuracy at step 630: 0.9637\n",
      "Accuracy at step 640: 0.9616\n",
      "Accuracy at step 650: 0.9618\n",
      "Accuracy at step 660: 0.9623\n",
      "Accuracy at step 670: 0.9604\n",
      "Accuracy at step 680: 0.9628\n",
      "Accuracy at step 690: 0.9615\n",
      "Adding run metadata for 699\n",
      "Accuracy at step 700: 0.9615\n",
      "Accuracy at step 710: 0.9625\n",
      "Accuracy at step 720: 0.9633\n",
      "Accuracy at step 730: 0.964\n",
      "Accuracy at step 740: 0.9634\n",
      "Accuracy at step 750: 0.9634\n",
      "Accuracy at step 760: 0.9652\n",
      "Accuracy at step 770: 0.9634\n",
      "Accuracy at step 780: 0.9632\n",
      "Accuracy at step 790: 0.9637\n",
      "Adding run metadata for 799\n",
      "Accuracy at step 800: 0.9647\n",
      "Accuracy at step 810: 0.9649\n",
      "Accuracy at step 820: 0.9658\n",
      "Accuracy at step 830: 0.9658\n",
      "Accuracy at step 840: 0.9639\n",
      "Accuracy at step 850: 0.9659\n",
      "Accuracy at step 860: 0.9672\n",
      "Accuracy at step 870: 0.9646\n",
      "Accuracy at step 880: 0.9674\n",
      "Accuracy at step 890: 0.9653\n",
      "Adding run metadata for 899\n",
      "Accuracy at step 900: 0.9647\n",
      "Accuracy at step 910: 0.9657\n",
      "Accuracy at step 920: 0.9676\n",
      "Accuracy at step 930: 0.9665\n",
      "Accuracy at step 940: 0.9687\n",
      "Accuracy at step 950: 0.9687\n",
      "Accuracy at step 960: 0.968\n",
      "Accuracy at step 970: 0.9697\n",
      "Accuracy at step 980: 0.9693\n",
      "Accuracy at step 990: 0.966\n",
      "Adding run metadata for 999\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs tfjob-master-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error from server (BadRequest): pod tfjob-master-0 does not have a host assigned\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl -n kubeflow exec -it tfjob-master-0 -- bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       READY     STATUS      RESTARTS   AGE\r\n",
      "named-cardinal-mariadb-0   1/1       Running     1          3h\r\n",
      "tfjob-master-0             0/1       Completed   0          2h\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ../7-distributed-tensorflow/solution-src\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfjob.kubeflow.org \"tfjobdist\" created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl create -f tfjob-dist.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.extensions \"tensorboard\" created\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl apply -f tensorboard-tfjob-dist.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                           READY     STATUS      RESTARTS   AGE\r\n",
      "named-cardinal-mariadb-0       1/1       Running     0          23m\r\n",
      "tensorboard-5f96cf6b45-rbw8d   1/1       Running     0          24m\r\n",
      "tfjob-master-0                 0/1       Completed   0          3h\r\n",
      "tfjobdist-master-0             1/1       Running     0          24m\r\n",
      "tfjobdist-ps-0                 0/1       Completed   0          24m\r\n",
      "tfjobdist-worker-0             1/1       Running     0          24m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-05-25 01:21:36.762429: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\r\n",
      "2019-05-25 01:21:36.881302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1405] Found device 0 with properties: \r\n",
      "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\r\n",
      "pciBusID: 2bde:00:00.0\r\n",
      "totalMemory: 11.17GiB freeMemory: 11.11GiB\r\n",
      "2019-05-25 01:21:36.881344: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1484] Adding visible gpu devices: 0\r\n",
      "2019-05-25 01:21:37.158675: I tensorflow/core/common_runtime/gpu/gpu_device.cc:965] Device interconnect StreamExecutor with strength 1 edge matrix:\r\n",
      "2019-05-25 01:21:37.158731: I tensorflow/core/common_runtime/gpu/gpu_device.cc:971]      0 \r\n",
      "2019-05-25 01:21:37.158740: I tensorflow/core/common_runtime/gpu/gpu_device.cc:984] 0:   N \r\n",
      "2019-05-25 01:21:37.159270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1097] Created TensorFlow device (/job:master/replica:0/task:0/device:GPU:0 with 10762 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 2bde:00:00.0, compute capability: 3.7)\r\n",
      "2019-05-25 01:21:37.286728: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job master -> {0 -> localhost:2222}\r\n",
      "2019-05-25 01:21:37.286769: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job ps -> {0 -> tfjobdist-ps-0.default.svc:2222}\r\n",
      "2019-05-25 01:21:37.286777: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:215] Initialize GrpcChannelCache for job worker -> {0 -> tfjobdist-worker-0.default.svc:2222}\r\n",
      "2019-05-25 01:21:37.289216: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:375] Started server with target: grpc://localhost:2222\r\n",
      "WARNING:tensorflow:From /app/main.py:60: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Please write your own downloading logic.\r\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Please use tf.data to implement this functionality.\r\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Please use tf.data to implement this functionality.\r\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Please use tf.one_hot on tensors.\r\n",
      "WARNING:tensorflow:From /usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: __init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\r\n",
      "WARNING:tensorflow:From /app/main.py:154: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "\r\n",
      "Future major versions of TensorFlow will allow gradients to flow\r\n",
      "into the labels input on backprop by default.\r\n",
      "\r\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\r\n",
      "\r\n",
      "WARNING:tensorflow:From /app/main.py:191: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\r\n",
      "Instructions for updating:\r\n",
      "Please switch to tf.train.MonitoredTrainingSession\r\n",
      "2019-05-25 01:21:48.504525: I tensorflow/core/distributed_runtime/master.cc:224] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n",
      "2019-05-25 01:21:48.504587: I tensorflow/core/distributed_runtime/master.cc:224] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\r\n",
      "2019-05-25 01:21:58.504689: I tensorflow/core/distributed_runtime/master.cc:224] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n",
      "2019-05-25 01:21:58.504735: I tensorflow/core/distributed_runtime/master.cc:224] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\r\n",
      "2019-05-25 01:22:08.504827: I tensorflow/core/distributed_runtime/master.cc:224] CreateSession still waiting for response from worker: /job:ps/replica:0/task:0\r\n",
      "2019-05-25 01:22:08.504877: I tensorflow/core/distributed_runtime/master.cc:224] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\r\n",
      "2019-05-25 01:22:18.504970: I tensorflow/core/distributed_runtime/master.cc:224] CreateSession still waiting for response from worker: /job:worker/replica:0/task:0\r\n",
      "2019-05-25 01:22:22.981821: I tensorflow/core/distributed_runtime/master_session.cc:1165] Start master session e684e933b19389ef with config: \r\n"
     ]
    }
   ],
   "source": [
    "!kubectl logs tfjobdist-master-0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: TYPE/NAME and list of ports are required for port-forward\r\n",
      "See 'kubectl port-forward -h' for help and examples.\r\n"
     ]
    }
   ],
   "source": [
    "# Run this in terminal\n",
    "\n",
    "!PODNAME=$(kubectl get pod -l app=tensorboard -o jsonpath='{.items[0].metadata.name}')\n",
    "!kubectl port-forward ${PODNAME} 6006:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deployment.extensions \"tensorboard\" deleted\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete -f tensorboard-tfjob-dist.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/federica/Documents/GitHub/kubeflow-labs/8-hyperparam-sweep/solution-chart\n"
     ]
    }
   ],
   "source": [
    "cd ../../8-hyperparam-sweep/solution-chart/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: error validating \"values.yaml\": error validating data: [apiVersion not set, kind not set]; if you choose to ignore these errors, turn validation off with --validate=false\r\n"
     ]
    }
   ],
   "source": [
    "# This did not work :( \n",
    "\n",
    "!kubectl create -f values.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:   lanky-bobcat\n",
      "LAST DEPLOYED: Fri May 24 21:55:21 2019\n",
      "NAMESPACE: default\n",
      "STATUS: DEPLOYED\n",
      "\n",
      "RESOURCES:\n",
      "==> v1/Pod(related)\n",
      "NAME                                 READY  STATUS             RESTARTS  AGE\n",
      "module8-tensorboard-57b9c9565-jz256  0/1    ContainerCreating  0         0s\n",
      "\n",
      "==> v1/Service\n",
      "NAME                 TYPE          CLUSTER-IP  EXTERNAL-IP  PORT(S)       AGE\n",
      "module8-tensorboard  LoadBalancer  10.0.38.97  <pending>    80:32302/TCP  1s\n",
      "\n",
      "==> v1beta1/Deployment\n",
      "NAME                 READY  UP-TO-DATE  AVAILABLE  AGE\n",
      "module8-tensorboard  0/1    1           0          1s\n",
      "\n",
      "==> v1beta2/TFJob\n",
      "NAME                  AGE\n",
      "module8-tf-paint-0-0  0s\n",
      "module8-tf-paint-0-1  0s\n",
      "module8-tf-paint-0-2  1s\n",
      "module8-tf-paint-1-0  0s\n",
      "module8-tf-paint-1-1  1s\n",
      "module8-tf-paint-1-2  0s\n",
      "module8-tf-paint-2-0  0s\n",
      "module8-tf-paint-2-1  0s\n",
      "module8-tf-paint-2-2  0s\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!helm install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                  READY     STATUS      RESTARTS   AGE\r\n",
      "module8-tensorboard-57b9c9565-jz256   1/1       Running     0          9m\r\n",
      "module8-tf-paint-0-0-master-0         0/1       Pending     0          9m\r\n",
      "module8-tf-paint-0-1-master-0         0/1       Pending     0          9m\r\n",
      "module8-tf-paint-0-2-master-0         1/1       Running     0          9m\r\n",
      "module8-tf-paint-1-0-master-0         0/1       Pending     0          9m\r\n",
      "module8-tf-paint-1-1-master-0         0/1       Pending     0          9m\r\n",
      "module8-tf-paint-1-2-master-0         0/1       Pending     0          9m\r\n",
      "module8-tf-paint-2-0-master-0         1/1       Running     0          9m\r\n",
      "module8-tf-paint-2-1-master-0         0/1       Pending     0          9m\r\n",
      "module8-tf-paint-2-2-master-0         0/1       Pending     0          9m\r\n",
      "named-cardinal-mariadb-0              1/1       Running     0          5m\r\n",
      "tfjob-master-0                        0/1       Completed   0          4h\r\n",
      "tfjobdist-ps-0                        0/1       Completed   0          43m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                            TYPE           CLUSTER-IP   EXTERNAL-IP     PORT(S)        AGE\r\n",
      "kubernetes                      ClusterIP      10.0.0.1     <none>          443/TCP        4h\r\n",
      "module8-tensorboard             LoadBalancer   10.0.38.97   13.68.227.187   80:32302/TCP   9m\r\n",
      "module8-tf-paint-0-0-master-0   ClusterIP      None         <none>          2222/TCP       9m\r\n",
      "module8-tf-paint-0-1-master-0   ClusterIP      None         <none>          2222/TCP       9m\r\n",
      "module8-tf-paint-0-2-master-0   ClusterIP      None         <none>          2222/TCP       9m\r\n",
      "module8-tf-paint-1-0-master-0   ClusterIP      None         <none>          2222/TCP       9m\r\n",
      "module8-tf-paint-1-1-master-0   ClusterIP      None         <none>          2222/TCP       9m\r\n",
      "module8-tf-paint-1-2-master-0   ClusterIP      None         <none>          2222/TCP       9m\r\n",
      "module8-tf-paint-2-0-master-0   ClusterIP      None         <none>          2222/TCP       9m\r\n",
      "module8-tf-paint-2-1-master-0   ClusterIP      None         <none>          2222/TCP       9m\r\n",
      "module8-tf-paint-2-2-master-0   ClusterIP      None         <none>          2222/TCP       9m\r\n",
      "tfjob-master-0                  ClusterIP      None         <none>          2222/TCP       4h\r\n",
      "tfjobdist-master-0              ClusterIP      None         <none>          2222/TCP       6m\r\n",
      "tfjobdist-ps-0                  ClusterIP      None         <none>          2222/TCP       43m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pod \"module8-tensorboard-57b9c9565-cndlj\" deleted\n",
      "pod \"named-cardinal-mariadb-0\" deleted\n"
     ]
    }
   ],
   "source": [
    "!kubectl delete pods --all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unable to connect to the server: dial tcp: lookup kubeflow-l-fenocerarg2-fb45cb-1cf097e8.hcp.eastus.azmk8s.io on [2001:4898::1050:5050]:53: no such host\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K - Finished ..\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Spin down Kubernetes cluster \n",
    "\n",
    "!az aks delete --yes --name {aks_name} --resource-group {resource_group}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Resource Group\n",
    "!az group create --name {resource_group} --location {location}\n",
    "# Create AKS cluster\n",
    "!az aks create --node-vm-size {agent_size} --resource-group {resource_group} --name {cluster_name} --location {location} --kubernetes-version 1.12.7 --node-count {agent_count} --generate-ssh-keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the kubeconfig file \n",
    "# Run this in terminal !az aks get-credentials --name {cluster_name} --resource-group {resource_group}\n",
    "\n",
    "# This is setuop for GPU \n",
    "# !kubectl apply -f https://raw.githubusercontent.com/nvidia/k8s-device-plugin/v1.11/nvidia-device-plugin.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                       STATUS    ROLES     AGE       VERSION\r\n",
      "aks-nodepool1-11457415-0   Ready     agent     6m        v1.12.7\r\n",
      "aks-nodepool1-11457415-1   Ready     agent     6m        v1.12.7\r\n",
      "aks-nodepool1-11457415-2   Ready     agent     6m        v1.12.7\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/federica/Documents/GitHub/kubeflow-labs/kubeflow\n"
     ]
    }
   ],
   "source": [
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "serviceaccount \"tiller\" created\n",
      "clusterrolebinding.rbac.authorization.k8s.io \"tiller\" created\n"
     ]
    }
   ],
   "source": [
    "# Setup Helm/Tiller\n",
    "!kubectl apply -f helm-rbac.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$HELM_HOME has been configured at /Users/federica/.helm.\n",
      "\n",
      "Tiller (the Helm server-side component) has been installed into your Kubernetes Cluster.\n",
      "\n",
      "Please note: by default, Tiller is deployed with an insecure 'allow unauthenticated users' policy.\n",
      "To prevent this, run `helm init` with the --tiller-tls-verify flag.\n",
      "For more information on securing your installation see: https://docs.helm.sh/using_helm/#securing-your-helm-installation\n",
      "Happy Helming!\n",
      "kube-system   tiller-deploy-7b65c7bff9-nwtsd          0/1       ContainerCreating   0          0s\n"
     ]
    }
   ],
   "source": [
    "!helm init --service-account tiller\n",
    "!kubectl get pod --all-namespaces | grep tiller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SETTING KUBEFLOW UP: \n",
    "# If don't have Kubeflow set up run this: \n",
    "\n",
    "# KUBEFLOW_SRC = \"Users/federica/Documents/Github/kubeflow-labs/kubeflow\"\n",
    "# !mkdir {KUBEFLOW_SRC}\n",
    "# !cd {KUBEFLOW_SRC}\n",
    "# !export KUBEFLOW_TAG=v0.4.1\n",
    "# !curl https://raw.githubusercontent.com/kubeflow/kubeflow/{KUBEFLOW_TAG}/scripts/download.sh | bash\n",
    "\n",
    "# have to go into the ksonnet/kubeflow app part before setting the param\n",
    "# cd ks_app\n",
    "# !ks param set jupyter serviceType LoadBalancer\n",
    "# cd ..\n",
    "# !../scripts/kfctl.sh apply k8s\n",
    "\n",
    "#USING KUBEFLOW: \n",
    "# To start a kubeflow deployment run this from inside the kubeflow app folder\n",
    "# ../scripts/kfctl.sh apply k8s\n",
    "\n",
    "\n",
    "# To delete kubeflow deployment run this from inside the kubeflow app folder \n",
    "# ../scripts/kfctl.sh delete all --delete_storage\n",
    "\n",
    "\n",
    "# To mount storage run: \n",
    "\n",
    "# !kubectl apply -f azure-file-sc.yaml\n",
    "# !kubectl apply -f azure-pvc-roles.yaml\n",
    "# !kubectl apply -f azure-file-pvc.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/federica/Documents/GitHub/kubeflow-labs/kubeflow\n"
     ]
    }
   ],
   "source": [
    "cd ../kubeflow/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/federica/Documents/GitHub/kubeflow-labs/kubeflow/mykubeflowapp\n"
     ]
    }
   ],
   "source": [
    "cd mykubeflowapp/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+ ENV_FILE=env.sh\n",
      "+ SKIP_INIT_PROJECT=false\n",
      "+ GKE_API_VERSION=v1beta1\n",
      "+ GCP_DEFAULT_ZONE=us-east1-d\n",
      "+++ dirname ../scripts/kfctl.sh\n",
      "++ cd ../scripts\n",
      "++ pwd\n",
      "+ DIR=/Users/federica/Documents/GitHub/kubeflow-labs/kubeflow/scripts\n",
      "+ source /Users/federica/Documents/GitHub/kubeflow-labs/kubeflow/scripts/util.sh\n",
      "+ source /Users/federica/Documents/GitHub/kubeflow-labs/kubeflow/scripts/gke/util.sh\n",
      "++ set -xe\n",
      "+ source /Users/federica/Documents/GitHub/kubeflow-labs/kubeflow/scripts/azure/util.sh\n",
      "++ set -xe\n",
      "+ source /Users/federica/Documents/GitHub/kubeflow-labs/kubeflow/scripts/aws/util.sh\n",
      "++ set -xe\n",
      "+ source /Users/federica/Documents/GitHub/kubeflow-labs/kubeflow/scripts/util-minikube.sh\n",
      "++ RED='\\033[0;31m'\n",
      "++ GREEN='\\033[0;32m'\n",
      "++ YELLOW='\\033[0;33m'\n",
      "++ NC='\\033[0m'\n",
      "++ MOUNT_LOCAL=false\n",
      "+ INPUT=()\n",
      "+ FORMAT=()\n",
      "+ export 'KUBEFLOW_COMPONENTS=\"ambassador\",\"jupyter\",\"notebook-controller\",\"jupyter-web-app\",\"profiles\",\"centraldashboard\",\"tf-job-operator\",\"pytorch-operator\",\"spartakus\",\"argo\",\"pipeline\"'\n",
      "+ KUBEFLOW_COMPONENTS='\"ambassador\",\"jupyter\",\"notebook-controller\",\"jupyter-web-app\",\"profiles\",\"centraldashboard\",\"tf-job-operator\",\"pytorch-operator\",\"spartakus\",\"argo\",\"pipeline\"'\n",
      "+ export KUBEFLOW_EXTENDEDINFO=false\n",
      "+ KUBEFLOW_EXTENDEDINFO=false\n",
      "+ which envsubst\n",
      "+ [[ 2 -lt 2 ]]\n",
      "+ COMMAND=apply\n",
      "+ WHAT=k8s\n",
      "+ shift\n",
      "+ shift\n",
      "+ main\n",
      "+ [[ apply == \\i\\n\\i\\t ]]\n",
      "+ source env.sh\n",
      "++ PLATFORM=null\n",
      "++ KUBEFLOW_REPO=/Users/federica/Documents/GitHub/kubeflow-labs/kubeflow\n",
      "++ KUBEFLOW_VERSION=master\n",
      "++ KUBEFLOW_COMPONENTS='\"ambassador\",\"jupyter\",\"notebook-controller\",\"jupyter-web-app\",\"profiles\",\"centraldashboard\",\"tf-job-operator\",\"pytorch-operator\",\"spartakus\",\"argo\",\"pipeline\"'\n",
      "++ KUBEFLOW_EXTENDEDINFO=false\n",
      "++ KUBEFLOW_KS_DIR=/Users/federica/Documents/GitHub/kubeflow-labs/kubeflow/mykubeflowapp/ks_app\n",
      "++ KUBEFLOW_DOCKER_REGISTRY=\n",
      "++ DOCKER_REGISTRY_KATIB_NAMESPACE=\n",
      "++ K8S_NAMESPACE=kubeflow\n",
      "++ KUBEFLOW_PLATFORM=null\n",
      "++ MOUNT_LOCAL=false\n",
      "++ DEPLOYMENT_NAME=mykubeflowapp\n",
      "+ [[ -z apply ]]\n",
      "+ [[ -z k8s ]]\n",
      "+ check_installed_deps\n",
      "+ kf_deps=(\"ks\" \"kubectl\")\n",
      "+ declare -a kf_deps\n",
      "+ kf_dep=ks\n",
      "+ min_ks_ver=0.11.0\n",
      "++ ks version\n",
      "++ cut '-d ' -f3\n",
      "++ head -1\n",
      "+ ks_ver=0.13.1\n",
      "+ '[' 0.13.1 '<' 0.11.0 ']'\n",
      "+ [[ null == \\g\\c\\p ]]\n",
      "+ [[ null == \\a\\z\\u\\r\\e ]]\n",
      "+ [[ null == \\a\\w\\s ]]\n",
      "+ [[ apply == \\g\\e\\n\\e\\r\\a\\t\\e ]]\n",
      "+ [[ apply == \\a\\p\\p\\l\\y ]]\n",
      "+ [[ k8s == \\p\\l\\a\\t\\f\\o\\r\\m ]]\n",
      "+ [[ k8s == \\a\\l\\l ]]\n",
      "+ [[ k8s == \\k\\8\\s ]]\n",
      "+ ksApply\n",
      "+ pushd /Users/federica/Documents/GitHub/kubeflow-labs/kubeflow/mykubeflowapp/ks_app\n",
      "~/Documents/GitHub/kubeflow-labs/kubeflow/mykubeflowapp/ks_app ~/Documents/GitHub/kubeflow-labs/kubeflow/mykubeflowapp\n",
      "+ createNamespace\n",
      "+ set +e\n",
      "++ kubectl get namespace kubeflow\n",
      "+ O='Error from server (NotFound): namespaces \"kubeflow\" not found'\n",
      "+ RESULT=1\n",
      "+ set -e\n",
      "+ [[ 1 -eq 0 ]]\n",
      "+ kubectl create namespace kubeflow\n",
      "namespace \"kubeflow\" created\n",
      "+ createKsEnv\n",
      "+ pushd /Users/federica/Documents/GitHub/kubeflow-labs/kubeflow/mykubeflowapp/ks_app\n",
      "~/Documents/GitHub/kubeflow-labs/kubeflow/mykubeflowapp/ks_app ~/Documents/GitHub/kubeflow-labs/kubeflow/mykubeflowapp/ks_app ~/Documents/GitHub/kubeflow-labs/kubeflow/mykubeflowapp\n",
      "+ set +e\n",
      "++ ks env describe default\n",
      "+ O='level=error msg=\"environment \\\"default\\\" was not found\"'\n",
      "+ RESULT=1\n",
      "+ set -e\n",
      "+ '[' 1 -eq 0 ']'\n",
      "+ ks env add default --namespace kubeflow\n",
      "\u001b[36mINFO\u001b[0m Using context \"kubeflow-labs-test\" from kubeconfig file \"/Users/federica/.kube/config\" \n",
      "\u001b[36mINFO\u001b[0m Creating environment \"default\" with namespace \"kubeflow\", pointing to \"version:v1.12.7\" cluster at address \"https://kubeflow-l-fenocerarg2-fb45cb-264930d6.hcp.eastus.azmk8s.io:443\" \n",
      "+ popd\n",
      "~/Documents/GitHub/kubeflow-labs/kubeflow/mykubeflowapp/ks_app ~/Documents/GitHub/kubeflow-labs/kubeflow/mykubeflowapp\n",
      "+ [[ null != \\m\\i\\n\\i\\k\\u\\b\\e ]]\n",
      "+ [[ null != \\d\\o\\c\\k\\e\\r\\-\\f\\o\\r\\-\\d\\e\\s\\k\\t\\o\\p ]]\n",
      "+ [[ -z '' ]]\n",
      "+ export 'KUBEFLOW_COMPONENTS+=,\"katib\"'\n",
      "+ KUBEFLOW_COMPONENTS+=',\"katib\"'\n",
      "+ writeEnv\n",
      "+ IFS=\n",
      "+ echo -e ''\n",
      "+ envsubst ''\n",
      "+ read line\n",
      "++ echo\n",
      "++ sed 's/\"/\\\\\"/g'\n",
      "+ line=\n",
      "+ eval echo\n",
      "++ echo\n",
      "+ read line\n",
      "+ ks param set application components '[\"ambassador\",\"jupyter\",\"notebook-controller\",\"jupyter-web-app\",\"profiles\",\"centraldashboard\",\"tf-job-operator\",\"pytorch-operator\",\"spartakus\",\"argo\",\"pipeline\",\"katib\"]'\n",
      "+ popd\n",
      "~/Documents/GitHub/kubeflow-labs/kubeflow/mykubeflowapp\n",
      "+ set +x\n",
      "+ [[ null == \\g\\c\\p ]]\n",
      "+ [[ null == \\a\\w\\s ]]\n",
      "+ pushd /Users/federica/Documents/GitHub/kubeflow-labs/kubeflow/mykubeflowapp/ks_app\n",
      "~/Documents/GitHub/kubeflow-labs/kubeflow/mykubeflowapp/ks_app ~/Documents/GitHub/kubeflow-labs/kubeflow/mykubeflowapp\n",
      "+ ks param set application name mykubeflowapp\n",
      "+ [[ false == true ]]\n",
      "+ ks param set application components '[\"ambassador\",\"jupyter\",\"notebook-controller\",\"jupyter-web-app\",\"profiles\",\"centraldashboard\",\"tf-job-operator\",\"pytorch-operator\",\"spartakus\",\"argo\",\"pipeline\",\"katib\"]'\n",
      "+ ks show default -c metacontroller -c application\n",
      "+ kubectl apply --validate=false -f default.yaml\n",
      "customresourcedefinition.apiextensions.k8s.io \"applications.app.k8s.io\" created\n",
      "customresourcedefinition.apiextensions.k8s.io \"compositecontrollers.metacontroller.k8s.io\" created\n",
      "customresourcedefinition.apiextensions.k8s.io \"controllerrevisions.metacontroller.k8s.io\" created\n",
      "customresourcedefinition.apiextensions.k8s.io \"decoratorcontrollers.metacontroller.k8s.io\" created\n",
      "customresourcedefinition.apiextensions.k8s.io \"notebooks.kubeflow.org\" created\n",
      "customresourcedefinition.apiextensions.k8s.io \"profiles.kubeflow.org\" created\n",
      "customresourcedefinition.apiextensions.k8s.io \"pytorchjobs.kubeflow.org\" created\n",
      "customresourcedefinition.apiextensions.k8s.io \"scheduledworkflows.kubeflow.org\" created\n",
      "customresourcedefinition.apiextensions.k8s.io \"studyjobs.kubeflow.org\" created\n",
      "customresourcedefinition.apiextensions.k8s.io \"tfjobs.kubeflow.org\" created\n",
      "customresourcedefinition.apiextensions.k8s.io \"viewers.kubeflow.org\" created\n",
      "customresourcedefinition.apiextensions.k8s.io \"workflows.argoproj.io\" created\n",
      "clusterrole.rbac.authorization.k8s.io \"centraldashboard\" created\n",
      "clusterrole.rbac.authorization.k8s.io \"jupyter-web-app-cluster-role\" created\n",
      "clusterrole.rbac.authorization.k8s.io \"katib-ui\" created\n",
      "clusterrole.rbac.authorization.k8s.io \"metrics-collector\" created\n",
      "clusterrole.rbac.authorization.k8s.io \"notebooks-controller\" created\n",
      "clusterrole.rbac.authorization.k8s.io \"studyjob-controller\" created\n",
      "clusterrole.rbac.authorization.k8s.io \"ambassador\" created\n",
      "clusterrole.rbac.authorization.k8s.io \"argo\" created\n",
      "clusterrole.rbac.authorization.k8s.io \"argo-ui\" created\n",
      "clusterrole.rbac.authorization.k8s.io \"ml-pipeline-persistenceagent\" created\n",
      "clusterrole.rbac.authorization.k8s.io \"ml-pipeline-viewer-controller-role\" created\n",
      "clusterrole.rbac.authorization.k8s.io \"pipeline-runner\" created\n",
      "clusterrole.rbac.authorization.k8s.io \"pytorch-operator\" created\n",
      "clusterrole.rbac.authorization.k8s.io \"spartakus\" created\n",
      "clusterrole.rbac.authorization.k8s.io \"tf-job-dashboard\" created\n",
      "clusterrole.rbac.authorization.k8s.io \"tf-job-operator\" created\n",
      "clusterrolebinding.rbac.authorization.k8s.io \"centraldashboard\" created\n",
      "clusterrolebinding.rbac.authorization.k8s.io \"jupyter-web-app-binding\" created\n",
      "clusterrolebinding.rbac.authorization.k8s.io \"katib-ui\" created\n",
      "clusterrolebinding.rbac.authorization.k8s.io \"meta-controller-cluster-role-binding\" created\n",
      "clusterrolebinding.rbac.authorization.k8s.io \"metrics-collector\" created\n",
      "clusterrolebinding.rbac.authorization.k8s.io \"notebooks-controller\" created\n",
      "clusterrolebinding.rbac.authorization.k8s.io \"profile-controller-cluster-role-binding\" created\n",
      "clusterrolebinding.rbac.authorization.k8s.io \"studyjob-controller\" created\n",
      "clusterrolebinding.rbac.authorization.k8s.io \"ambassador\" created\n",
      "clusterrolebinding.rbac.authorization.k8s.io \"argo\" created\n",
      "clusterrolebinding.rbac.authorization.k8s.io \"argo-ui\" created\n",
      "clusterrolebinding.rbac.authorization.k8s.io \"ml-pipeline-persistenceagent\" created\n",
      "clusterrolebinding.rbac.authorization.k8s.io \"ml-pipeline-scheduledworkflow\" created\n",
      "clusterrolebinding.rbac.authorization.k8s.io \"ml-pipeline-viewer-crd-role-binding\" created\n",
      "clusterrolebinding.rbac.authorization.k8s.io \"pipeline-runner\" created\n",
      "clusterrolebinding.rbac.authorization.k8s.io \"pytorch-operator\" created\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clusterrolebinding.rbac.authorization.k8s.io \"spartakus\" created\n",
      "clusterrolebinding.rbac.authorization.k8s.io \"tf-job-dashboard\" created\n",
      "clusterrolebinding.rbac.authorization.k8s.io \"tf-job-operator\" created\n",
      "compositecontroller.metacontroller.k8s.io \"mykubeflowapp-controller\" created\n",
      "secret \"mlpipeline-minio-artifact\" created\n",
      "secret \"vizier-db-secrets\" created\n",
      "configmap \"jupyter-config\" created\n",
      "configmap \"jupyter-web-app-config\" created\n",
      "configmap \"metricscollector-template\" created\n",
      "configmap \"mykubeflowapp-controller-hooks\" created\n",
      "configmap \"pytorch-operator-config\" created\n",
      "configmap \"tf-job-operator-config\" created\n",
      "configmap \"worker-template\" created\n",
      "configmap \"workflow-controller-configmap\" created\n",
      "persistentvolumeclaim \"katib-mysql\" created\n",
      "persistentvolumeclaim \"minio-pvc\" created\n",
      "persistentvolumeclaim \"mysql-pv-claim\" created\n",
      "serviceaccount \"ambassador\" created\n",
      "serviceaccount \"argo\" created\n",
      "serviceaccount \"argo-ui\" created\n",
      "serviceaccount \"centraldashboard\" created\n",
      "serviceaccount \"default-editor\" created\n",
      "serviceaccount \"jupyter\" created\n",
      "serviceaccount \"jupyter-notebook\" created\n",
      "serviceaccount \"jupyter-web-app\" created\n",
      "serviceaccount \"katib-ui\" created\n",
      "serviceaccount \"meta-controller-service\" created\n",
      "serviceaccount \"metrics-collector\" created\n",
      "serviceaccount \"ml-pipeline\" created\n",
      "serviceaccount \"ml-pipeline-persistenceagent\" created\n",
      "serviceaccount \"ml-pipeline-scheduledworkflow\" created\n",
      "serviceaccount \"ml-pipeline-ui\" created\n",
      "serviceaccount \"ml-pipeline-viewer-crd-service-account\" created\n",
      "serviceaccount \"notebook-controller\" created\n",
      "serviceaccount \"pipeline-runner\" created\n",
      "serviceaccount \"profiles\" created\n",
      "serviceaccount \"pytorch-operator\" created\n",
      "serviceaccount \"spartakus\" created\n",
      "serviceaccount \"studyjob-controller\" created\n",
      "serviceaccount \"tf-job-dashboard\" created\n",
      "serviceaccount \"tf-job-operator\" created\n",
      "role.rbac.authorization.k8s.io \"profiles\" created\n",
      "role.rbac.authorization.k8s.io \"centraldashboard\" created\n",
      "role.rbac.authorization.k8s.io \"jupyter-notebook-role\" created\n",
      "role.rbac.authorization.k8s.io \"jupyter-role\" created\n",
      "role.rbac.authorization.k8s.io \"ml-pipeline\" created\n",
      "role.rbac.authorization.k8s.io \"ml-pipeline-scheduledworkflow\" created\n",
      "role.rbac.authorization.k8s.io \"ml-pipeline-ui\" created\n",
      "rolebinding.rbac.authorization.k8s.io \"profiles\" created\n",
      "rolebinding.rbac.authorization.k8s.io \"centraldashboard\" created\n",
      "rolebinding.rbac.authorization.k8s.io \"default-editor-role-binding\" created\n",
      "rolebinding.rbac.authorization.k8s.io \"jupyter-notebook-role\" created\n",
      "rolebinding.rbac.authorization.k8s.io \"jupyter-role\" created\n",
      "rolebinding.rbac.authorization.k8s.io \"ml-pipeline\" created\n",
      "rolebinding.rbac.authorization.k8s.io \"ml-pipeline-ui\" created\n",
      "service \"ambassador\" created\n",
      "service \"ambassador-admin\" created\n",
      "service \"argo-ui\" created\n",
      "service \"centraldashboard\" created\n",
      "service \"jupyter-0\" created\n",
      "service \"jupyter-lb\" created\n",
      "service \"jupyter-web-app\" created\n",
      "service \"katib-ui\" created\n",
      "service \"minio-service\" created\n",
      "service \"ml-pipeline\" created\n",
      "service \"ml-pipeline-tensorboard-ui\" created\n",
      "service \"ml-pipeline-ui\" created\n",
      "service \"mykubeflowapp-controller\" created\n",
      "service \"mysql\" created\n",
      "service \"notebooks-controller\" created\n",
      "service \"profiles\" created\n",
      "service \"studyjob-controller\" created\n",
      "service \"tf-job-dashboard\" created\n",
      "service \"vizier-core\" created\n",
      "service \"vizier-core-rest\" created\n",
      "service \"vizier-db\" created\n",
      "service \"vizier-suggestion-bayesianoptimization\" created\n",
      "service \"vizier-suggestion-grid\" created\n",
      "service \"vizier-suggestion-hyperband\" created\n",
      "service \"vizier-suggestion-random\" created\n",
      "deployment.apps \"jupyter-web-app\" created\n",
      "deployment.apps \"profiles\" created\n",
      "deployment.apps \"ambassador\" created\n",
      "deployment.apps \"minio\" created\n",
      "deployment.apps \"mykubeflowapp-controller\" created\n",
      "deployment.apps \"notebooks-controller\" created\n",
      "deployment.apps \"ml-pipeline\" created\n",
      "deployment.apps \"ml-pipeline-persistenceagent\" created\n",
      "deployment.apps \"ml-pipeline-scheduledworkflow\" created\n",
      "deployment.apps \"ml-pipeline-ui\" created\n",
      "deployment.apps \"ml-pipeline-viewer-controller-deployment\" created\n",
      "deployment.apps \"mysql\" created\n",
      "deployment.extensions \"argo-ui\" created\n",
      "deployment.extensions \"centraldashboard\" created\n",
      "deployment.extensions \"katib-ui\" created\n",
      "deployment.extensions \"pytorch-operator\" created\n",
      "deployment.extensions \"spartakus-volunteer\" created\n",
      "deployment.extensions \"studyjob-controller\" created\n",
      "deployment.extensions \"tf-job-dashboard\" created\n",
      "deployment.extensions \"tf-job-operator\" created\n",
      "deployment.extensions \"vizier-core\" created\n",
      "deployment.extensions \"vizier-core-rest\" created\n",
      "deployment.extensions \"vizier-db\" created\n",
      "deployment.extensions \"vizier-suggestion-bayesianoptimization\" created\n",
      "deployment.extensions \"vizier-suggestion-grid\" created\n",
      "deployment.extensions \"vizier-suggestion-hyperband\" created\n",
      "deployment.extensions \"vizier-suggestion-random\" created\n",
      "deployment.extensions \"workflow-controller\" created\n",
      "statefulset.apps \"jupyter\" created\n",
      "statefulset.apps \"metacontroller\" created\n",
      "application.app.k8s.io \"mykubeflowapp\" created\n",
      "+ popd\n",
      "~/Documents/GitHub/kubeflow-labs/kubeflow/mykubeflowapp\n",
      "+ [[ apply == \\d\\e\\l\\e\\t\\e ]]\n"
     ]
    }
   ],
   "source": [
    "# Creating the kubeflow instance \n",
    "!../scripts/kfctl.sh apply k8s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                                        READY     STATUS    RESTARTS   AGE\r\n",
      "ambassador-5cdff47f4d-mvw4b                                 1/1       Running   0          4m\r\n",
      "ambassador-5cdff47f4d-ssj6x                                 1/1       Running   0          4m\r\n",
      "ambassador-5cdff47f4d-vswk2                                 1/1       Running   0          4m\r\n",
      "argo-ui-789c8577d5-hwxsn                                    1/1       Running   0          4m\r\n",
      "centraldashboard-6f9948dc6d-txrr8                           1/1       Running   0          4m\r\n",
      "jupyter-0                                                   1/1       Running   0          4m\r\n",
      "jupyter-web-app-5c64c4f4cb-xthpm                            1/1       Running   0          4m\r\n",
      "katib-ui-78f445bf8f-r2vzn                                   1/1       Running   0          4m\r\n",
      "metacontroller-0                                            1/1       Running   0          4m\r\n",
      "minio-648694bc46-hvqt2                                      1/1       Running   0          4m\r\n",
      "ml-pipeline-6c5d5ddbdf-drsz8                                1/1       Running   0          4m\r\n",
      "ml-pipeline-persistenceagent-5657c6f4bd-9jkvv               1/1       Running   1          4m\r\n",
      "ml-pipeline-scheduledworkflow-6fd8894f86-jc6jz              1/1       Running   0          4m\r\n",
      "ml-pipeline-ui-65868fd6cb-gkfv6                             1/1       Running   0          4m\r\n",
      "ml-pipeline-viewer-controller-deployment-6c75f9bbfb-88mdh   1/1       Running   0          4m\r\n",
      "mykubeflowapp-controller-579bd7848f-5kzwf                   1/1       Running   0          4m\r\n",
      "mysql-6f5c96c765-c4gnw                                      1/1       Running   0          4m\r\n",
      "notebooks-controller-69b6bf4854-8z6wr                       1/1       Running   0          4m\r\n",
      "profiles-77f98bb9cd-s5mb5                                   1/1       Running   0          4m\r\n",
      "pytorch-operator-78cc45b958-v5wf7                           1/1       Running   0          4m\r\n",
      "spartakus-volunteer-7b57fcb8bb-ng7rd                        1/1       Running   0          4m\r\n",
      "studyjob-controller-559f688558-knff7                        1/1       Running   0          4m\r\n",
      "tf-job-dashboard-8fc7bfbc7-dtl25                            1/1       Running   0          4m\r\n",
      "tf-job-operator-58fd8db6bd-f68nx                            1/1       Running   0          4m\r\n",
      "vizier-core-5dc4594c4c-ktf97                                0/1       Running   2          4m\r\n",
      "vizier-core-rest-5fcf9bccf-ns624                            1/1       Running   0          4m\r\n",
      "vizier-db-cf8cfdc79-w7b9z                                   0/1       Running   0          4m\r\n",
      "vizier-suggestion-bayesianoptimization-67f69667b9-49f2f     1/1       Running   0          4m\r\n",
      "vizier-suggestion-grid-85f9f9d977-w9zdt                     1/1       Running   0          4m\r\n",
      "vizier-suggestion-hyperband-8bcc744b5-v84mw                 1/1       Running   0          4m\r\n",
      "vizier-suggestion-random-6d65dd6645-6xn5p                   1/1       Running   0          4m\r\n",
      "workflow-controller-659f5c5dfd-cwtqz                        1/1       Running   0          4m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods -n kubeflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# have to go into the ksonnet/kubeflow app part before setting the param\n",
    "# cd ks_app\n",
    "# !ks param set jupyter serviceType LoadBalancer\n",
    "# cd ..\n",
    "# !kubeflow/scripts/kfctl.sh apply k8s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azure-file-pvc.yaml   helm-rbac.yaml        \u001b[34mscripts\u001b[m\u001b[m/\r\n",
      "azure-file-sc.yaml    \u001b[34mhyperparam-chart\u001b[m\u001b[m/     tfjob.yaml\r\n",
      "azure-pvc-roles.yaml  \u001b[34mkubeflow\u001b[m\u001b[m/\r\n",
      "\u001b[34mdeployment\u001b[m\u001b[m/           \u001b[34mmykubeflowapp\u001b[m\u001b[m/\r\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME:   impressive-hydra\n",
      "LAST DEPLOYED: Thu May 30 11:22:33 2019\n",
      "NAMESPACE: default\n",
      "STATUS: DEPLOYED\n",
      "\n",
      "RESOURCES:\n",
      "==> v1/Pod(related)\n",
      "NAME                                 READY  STATUS             RESTARTS  AGE\n",
      "module8-tensorboard-57b9c9565-vnz2h  0/1    ContainerCreating  0         0s\n",
      "\n",
      "==> v1/Service\n",
      "NAME                 TYPE          CLUSTER-IP   EXTERNAL-IP  PORT(S)       AGE\n",
      "module8-tensorboard  LoadBalancer  10.0.115.45  <pending>    80:31190/TCP  1s\n",
      "\n",
      "==> v1beta1/Deployment\n",
      "NAME                 READY  UP-TO-DATE  AVAILABLE  AGE\n",
      "module8-tensorboard  0/1    1           0          1s\n",
      "\n",
      "==> v1beta2/TFJob\n",
      "NAME                  AGE\n",
      "module8-tf-paint-0-0  0s\n",
      "module8-tf-paint-0-1  0s\n",
      "module8-tf-paint-0-2  0s\n",
      "module8-tf-paint-1-0  0s\n",
      "module8-tf-paint-1-1  1s\n",
      "module8-tf-paint-1-2  0s\n",
      "module8-tf-paint-2-0  1s\n",
      "module8-tf-paint-2-1  0s\n",
      "module8-tf-paint-2-2  0s\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# We now have to deploy our helm charts for the parameter sweep to our cluster. \n",
    "\n",
    "!helm install hyperparam-chart/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1.12.7\"\r\n"
     ]
    }
   ],
   "source": [
    "!az aks show --resource-group fenocera_rg_2 --name kubeflow-labs-test --query kubernetesVersion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                  DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\r\n",
      "module8-tensorboard   1         1         1            1           3m\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This did not work!! \n",
    "\n",
    "# !kubectl autoscale deployment module8-tensorboard --cpu-percent=50 --min=1 --max=10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                                  READY     STATUS      RESTARTS   AGE\n",
      "module8-tensorboard-57b9c9565-vnz2h   1/1       Running     0          56m\n",
      "module8-tf-paint-0-0-master-0         0/1       Completed   0          56m\n",
      "module8-tf-paint-0-1-master-0         0/1       Completed   0          56m\n",
      "module8-tf-paint-0-2-master-0         0/1       Completed   0          56m\n",
      "module8-tf-paint-1-0-master-0         0/1       Completed   0          56m\n",
      "module8-tf-paint-1-1-master-0         0/1       Completed   0          56m\n",
      "module8-tf-paint-1-2-master-0         0/1       Completed   0          56m\n",
      "module8-tf-paint-2-0-master-0         0/1       Completed   0          56m\n",
      "module8-tf-paint-2-1-master-0         0/1       Completed   0          56m\n",
      "module8-tf-paint-2-2-master-0         0/1       Completed   0          56m\n"
     ]
    }
   ],
   "source": [
    "!kubectl get pods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                            TYPE           CLUSTER-IP    EXTERNAL-IP     PORT(S)        AGE\r\n",
      "kubernetes                      ClusterIP      10.0.0.1      <none>          443/TCP        2h\r\n",
      "module8-tensorboard             LoadBalancer   10.0.115.45   104.45.133.25   80:31190/TCP   1h\r\n",
      "module8-tf-paint-0-0-master-0   ClusterIP      None          <none>          2222/TCP       1h\r\n",
      "module8-tf-paint-0-1-master-0   ClusterIP      None          <none>          2222/TCP       1h\r\n",
      "module8-tf-paint-0-2-master-0   ClusterIP      None          <none>          2222/TCP       1h\r\n",
      "module8-tf-paint-1-0-master-0   ClusterIP      None          <none>          2222/TCP       1h\r\n",
      "module8-tf-paint-1-1-master-0   ClusterIP      None          <none>          2222/TCP       1h\r\n",
      "module8-tf-paint-1-2-master-0   ClusterIP      None          <none>          2222/TCP       1h\r\n",
      "module8-tf-paint-2-0-master-0   ClusterIP      None          <none>          2222/TCP       1h\r\n",
      "module8-tf-paint-2-1-master-0   ClusterIP      None          <none>          2222/TCP       1h\r\n",
      "module8-tf-paint-2-2-master-0   ClusterIP      None          <none>          2222/TCP       1h\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get service"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME                  DESIRED   CURRENT   UP-TO-DATE   AVAILABLE   AGE\r\n",
      "module8-tensorboard   1         1         1            1           1h\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl get deployments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: the server doesn't have a resource type \"module8-tf-paint-1-0-master-0\"\r\n"
     ]
    }
   ],
   "source": [
    "!kubectl describe module8-tf-paint-1-0-master-0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b]0;root@module8-tf-paint-0-0-master-0: /app\u0007root@module8-tf-paint-0-0-master-0:/app# "
     ]
    }
   ],
   "source": [
    "!kubectl exec -it module8-tf-paint-0-0-master-0 -- bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAME            \tREVISION\tUPDATED                 \tSTATUS  \tCHART                         \tAPP VERSION\tNAMESPACE\r\n",
      "impressive-hydra\t1       \tThu May 30 11:22:33 2019\tDEPLOYED\tmodule8-hyperparam-sweep-0.1.0\t           \tdefault  \r\n"
     ]
    }
   ],
   "source": [
    "!helm list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "release \"impressive-hydra\" deleted\r\n"
     ]
    }
   ],
   "source": [
    "# delete helm deployment \n",
    "\n",
    "!helm delete --purge impressive-hydra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Serving the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ACCESS_SECRET_KEY=hDrossgmaLFd7C6bRYqrQ6w8XTn+o2pQxpkhfKoyieD3mKKjkmesSpW7zpFxPkIze9R711pbOO1A0wjw1LQFJQ==\n",
    "# ACCESS_KEY=f2c8637c082ee11e9a394d6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/sh: stable/minio: No such file or directory\r\n"
     ]
    }
   ],
   "source": [
    "# Ran this in terminal \n",
    "# helm install --name minio --set accessKey=$ACCESS_KEY,secretKey=$ACCESS_SECRET_KEY,service.type=LoadBalancer stable/minio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then run this in terminal \n",
    "\n",
    "# SERVICE_IP=$(kubectl get svc minio --template=\"{{range .status.loadBalancer.ingress}}{{.ip}}{{end}}\")\n",
    "# S3_ENDPOINT=http://${SERVICE_IP}:9000   ##THIS WAS WRONG IN THE TUTORIAL \n",
    "\n",
    "# Followed by \n",
    "\n",
    "# mc config host add minio $S3_ENDPOINT $ACCESS_KEY $ACCESS_SECRET_KEY\n",
    "\n",
    "# Finally: \n",
    "\n",
    "# BUCKET_NAME=kubeflow\n",
    "\n",
    "# mc mb minio/$BUCKET_NAME\n",
    "\n",
    "# mc cp --recursive ../model_tf/$BUCKET_NAME # THIS IS WRONG IT NEEDS A SPACE\n",
    "# ie: \n",
    "# mc cp --recursive model_tf/ $BUCKET_NAME\n",
    "\n",
    "\n",
    "\n",
    "# export NAMESPACE=serving\n",
    "\n",
    "# kubectl create namespace $NAMESPACE\n",
    "\n",
    "\n",
    "# kubectl create secret generic serving-creds --from-literal=accessKeyID=${ACCESS_KEY} \\\n",
    "#  --from-literal=secretAccessKey=${ACCESS_SECRET_KEY} -n $NAMESPACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S3_USE_HTTPS=0\n",
    "S3_VERIFY_SSL=0\n",
    "JOB_NAME=myjob\n",
    "MODEL_COMPONENT=serveInception\n",
    "MODEL_NAME=inception\n",
    "MODEL_PATH=s3://${BUCKET_NAME}/models/${JOB_NAME}/export/${MODEL_NAME}/\n",
    "MODEL_SERVER_IMAGE=sozercan/tensorflow-model-server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_SERVER_IMAGE=tensorflow/serving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K - Finished ..\u001b[0m"
     ]
    }
   ],
   "source": [
    "!az aks delete --yes --name {aks_name} --resource-group {resource_group}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wine_env",
   "language": "python",
   "name": "wine_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
